{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92d84e1f8068fa1",
   "metadata": {},
   "source": [
    "## Sentence Reconstruction Using NLP Techniques\n",
    "\n",
    "This Jupyter notebook implements a no-library Python pipeline to reconstruct two ambiguous sentences using basic Natural Language Processing (NLP) techniques. The goal is to clarify the meaning of the sentences by addressing ambiguities through tokenization, part-of-speech (POS) tagging, custom transformation rules, and sentence reconstruction. Each step is explained with the underlying NLP theory to provide a comprehensive understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2af30569f6719",
   "metadata": {},
   "source": [
    "### 1. Introduction to NLP and the Task\n",
    "\n",
    "Theory: What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a field of computer science that focuses on enabling computers to understand and process human language. It involves techniques such as tokenization, POS tagging, parsing, and grammars to analyze and manipulate text. In this task, we use these techniques to reconstruct two sentences:\n",
    "- Sentence 1: \"Thank your message to show our words to the doctor, as his next contract checking, to all of us.\"\n",
    "- Sentence 2: \"Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets.\"\n",
    "\n",
    "The sentences contain ambiguities, such as unclear phrasing (\"Thank your message\") and vague modifiers (\"as his next contract checking\"). Our pipeline aims to rephrase them for clarity using a rule-based approach.\n",
    "\n",
    "**Approach**:\n",
    "The pipeline consists of:\n",
    "1. Tokenization: Breaking sentences into tokens (words and punctuation).\n",
    "2. POS Tagging: Assigning grammatical categories to tokens.\n",
    "3. Transformation Rules: Defining rules to rephrase ambiguous segments\n",
    "4. Automaton: Applying rules to transform the tagged tokens.\n",
    "5. Reconstruction: Reassembling tokens into clearer sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4666f51a0b5e156",
   "metadata": {},
   "source": [
    "### 2. Tokenization\n",
    "\n",
    "#### **Theory: What is Tokenization?**\n",
    "Tokenization is the process of splitting text into smaller units called tokens, typically words, punctuation, or symbols. It’s a foundational step in NLP, enabling further analysis like tagging or parsing. Tokenization must handle punctuation, spaces, and special cases (e.g., contractions or acronyms) appropriately.\n",
    "\n",
    "**Implementation**\n",
    "Without external libraries, we implement a simple tokenizer that:\n",
    "- Iterates through each character in the sentence.\n",
    "- Groups alphanumeric characters into words.\n",
    "- Treats punctuation as separate tokens.\n",
    "- Handles spaces to separate words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:36:36.040045Z",
     "start_time": "2025-04-24T18:36:36.034939Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    word = \"\"\n",
    "    for char in sentence:\n",
    "        if char.isalnum():\n",
    "            word += char\n",
    "        else:\n",
    "            if word:\n",
    "                tokens.append(word.lower())  # Case-insensitive\n",
    "                word = \"\"\n",
    "            if char.strip():  # Non-whitespace\n",
    "                tokens.append(char)\n",
    "    if word:\n",
    "        tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cc69d4f29061e",
   "metadata": {},
   "source": [
    "### **How It Works**\n",
    "- Input: A string (e.g., \"Thank your message,\").\n",
    "- Process: The function scans each character, building words from alphanumeric sequences and separating punctuation.\n",
    "- Output: A list of tokens (e.g., [\"thank\", \"your\", \"message\", \",\"]).\n",
    "- Example: For \"doctor,\", it produces [\"doctor\", \",\"].\n",
    "\n",
    "This approach is sufficient for the given sentences, which lack complex cases like contractions or acronyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19947f830dfbf8",
   "metadata": {},
   "source": [
    "### 3. Part-of-Speech (POS) Tagging\n",
    "#### **Theory: What is POS Tagging?**\n",
    "POS tagging assigns grammatical categories (e.g., noun, verb, adjective) to each token. It’s crucial for understanding sentence structure and disambiguating meanings. Common approaches include rule-based, statistical, or neural taggers, but without libraries, we use a dictionary-based method with context rules for ambiguous words.\n",
    "\n",
    "#### **Implementation**\n",
    "We define a dictionary mapping words to POS tags based on the sentences’ vocabulary. For ambiguous words like \"to,\" we use context to decide between preposition (PREP) or infinitive marker (TO).\n",
    "**POS Tags Used:**\n",
    "tag | Description\n",
    "---|---\n",
    "N | Noun\n",
    "V | Verb\n",
    "DET | Determiner\n",
    "PRON | Pronoun\n",
    "PREP | Preposition\n",
    "ADJ | Adjective\n",
    "ADV | Adverb\n",
    "CONJ | Conjunction\n",
    "PUNCT | Punctuation\n",
    "TO | Infinitive marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19d813dc2e65da9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:36:36.068663Z",
     "start_time": "2025-04-24T18:36:36.063084Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_dict = {\n",
    "    \"thank\": \"V\", \"your\": \"PRON\", \"message\": \"N\", \"to\": \"PREP\", \"show\": \"V\",\n",
    "    \"our\": \"PRON\", \"words\": \"N\", \"the\": \"DET\", \"doctor\": \"N\", \"as\": \"PREP\",\n",
    "    \"his\": \"PRON\", \"next\": \"ADJ\", \"contract\": \"N\", \"checking\": \"V\", \"all\": \"DET\",\n",
    "    \"of\": \"PREP\", \"us\": \"PRON\", \"overall\": \"ADV\", \"let\": \"V\", \"make\": \"V\",\n",
    "    \"sure\": \"ADJ\", \"are\": \"V\", \"safe\": \"ADJ\", \"and\": \"CONJ\", \"celebrate\": \"V\",\n",
    "    \"outcome\": \"N\", \"with\": \"PREP\", \"strong\": \"ADJ\", \"coffee\": \"N\", \"future\": \"ADJ\",\n",
    "    \"targets\": \"N\", \",\": \"PUNCT\", \".\": \"PUNCT\"\n",
    "}\n",
    "verbs = {word for word, tag in pos_dict.items() if tag == \"V\"}\n",
    "\n",
    "def tag(tokens):\n",
    "    tagged = [(token, pos_dict.get(token, \"UNK\")) for token in tokens]\n",
    "    for i in range(len(tagged) - 1):\n",
    "        if tagged[i][0] == \"to\":\n",
    "            tagged[i] = (\"to\", \"TO\" if tagged[i+1][1] == \"V\" else \"PREP\")\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed2ba30694c980",
   "metadata": {},
   "source": [
    "### **How It Works**:\n",
    "- Input: List of tokens (e.g., [\"thank\", \"your\", \"message\", \"to\", \"show\"]).\n",
    "- Process:\n",
    "    - Assigns tags from `pos_dict`.\n",
    "    - For \"to,\" checks if the next token is a verb to assign \"TO\" or \"PREP\".\n",
    "- Output: List of (word, tag) tuples (e.g., [(\"thank\", \"V\"), (\"your\", \"PRON\"), (\"message\", \"N\"), (\"to\", \"TO\"), (\"show\", \"V\")]).\n",
    "- Example: For the first sentence, \"to\" before \"show\" is tagged as \"TO,\" while \"to\" before \"the doctor\" is \"PREP.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d0df60dc7213d",
   "metadata": {},
   "source": [
    "### **4. Custom Grammar and Transformation Rules**\n",
    "#### **Theory: Grammars in NLP**\n",
    "A grammar defines the syntactic rules of a language, often using production rules (e.g., S → NP VP). In this task, instead of a full context-free grammar, we use transformation rules that act as a custom grammar to rephrase ambiguous segments. These rules are pattern-based, matching sequences of tagged tokens and replacing them with clearer alternatives.\n",
    "#### **Implementation**\n",
    "We define rules to address specific ambiguities in the sentences:\n",
    "- Rule 1: Insert \"you\" after \"thank.\"\n",
    "- Rule 2: Insert \"for\" before \"your message.\"\n",
    "- Rule 3: Replace \"as\" with \"during.\"\n",
    "- Rule 4: Replace \"checking\" with \"review.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307a45210e787d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:36:36.081341Z",
     "start_time": "2025-04-24T18:36:36.077182Z"
    }
   },
   "outputs": [],
   "source": [
    "rules = [\n",
    "    # Rule 1: Insert \"you\" after \"thank\"\n",
    "    (\n",
    "        [(\"thank\", \"V\")],\n",
    "        [(\"thank\", \"V\"), (\"you\", \"PRON\")]\n",
    "    ),\n",
    "    # Rule 2: Insert \"for\" before \"your message\"\n",
    "    (\n",
    "        [(\"you\", \"PRON\"), (\"your\", \"PRON\"), (\"message\", \"N\")],\n",
    "        [(\"you\", \"PRON\"), (\"for\", \"PREP\"), (\"your\", \"PRON\"), (\"message\", \"N\")]\n",
    "    ),\n",
    "    # Rule 3: Replace \"as\" with \"during\"\n",
    "    (\n",
    "        [(\"as\", \"PREP\")],\n",
    "        [(\"during\", \"PREP\")]\n",
    "    ),\n",
    "    # Rule 4: Replace \"checking\" with \"review\"\n",
    "    (\n",
    "        [(\"checking\", \"V\")],\n",
    "        [(\"review\", \"N\")]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b22735b576baf",
   "metadata": {},
   "source": [
    "**How It Works**\n",
    "- Each rule is a tuple of (pattern, replacement), where both are lists of (word, tag) pairs.\n",
    "- Patterns match specific sequences in the tagged sentence.\n",
    "- Replacements provide clearer phrasing with appropriate tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3695af202a1f0",
   "metadata": {},
   "source": [
    "### **5. Rule-Based Automaton**\n",
    "#### **Theory: Automata in NLP**\n",
    "An automaton is a computational model that processes input sequentially, often used in NLP for tasks like tokenization or parsing. Here, we implement a simple automaton as a loop that scans the tagged tokens, identifies patterns matching our rules, and applies the corresponding replacements.\n",
    "#### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d72130bf8491b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:36:36.103482Z",
     "start_time": "2025-04-24T18:36:36.093968Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_rules(tagged):\n",
    "    for pattern, replacement in rules:\n",
    "        pattern_len = len(pattern)\n",
    "        for i in range(len(tagged) - pattern_len + 1):\n",
    "            if tagged[i:i+pattern_len] == pattern:\n",
    "                tagged = tagged[:i] + replacement + tagged[i+pattern_len:]\n",
    "                break  # Apply each rule once per pattern\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8808472dbc9141a",
   "metadata": {},
   "source": [
    "### **How It Works**:\n",
    "- Input: List of tagged tokens.\n",
    "- Process: Scans for subsequences matching rule patterns and replaces them with the specified replacements.\n",
    "- Output: Transformed list of tagged tokens.\n",
    "- Example: For [(\"thank\", \"V\"), (\"your\", \"PRON\"), (\"message\", \"N\"), ...], it replaces the first three tokens with [(\"thank\", \"V\"), (\"you\", \"PRON\"), (\"for\", \"PREP\"), (\"your\", \"PRON\"), (\"message\", \"N\")]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09ff7105eac362",
   "metadata": {},
   "source": [
    "### **6. Sentence Reconstruction**\n",
    "#### **Theory: Reconstructing Sentences**\n",
    "After transforming the tokens, we reassemble them into a coherent sentence. This involves handling spacing and punctuation correctly to produce readable output.\n",
    "#### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a07b296b4468e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:36:36.656954Z",
     "start_time": "2025-04-24T18:36:36.651990Z"
    }
   },
   "outputs": [],
   "source": [
    "def reconstruct(tagged):\n",
    "    tokens = [word for word, _ in tagged]\n",
    "    sentence = \" \".join(tokens)\n",
    "    for punct in [\",\", \".\", \"!\", \"?\", \";\", \":\"]:\n",
    "        sentence = sentence.replace(\" \" + punct, punct)\n",
    "    return sentence[0].upper() + sentence[1:]  # Capitalize first letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba4db1e7262d11",
   "metadata": {},
   "source": [
    "### **How It Works**:\n",
    "- Input: List of tagged tokens.\n",
    "- Process: Extracts words, joins with spaces, removes spaces before punctuation, and capitalizes the first letter.\n",
    "- Output: A readable sentence.\n",
    "- Example: For [\"thank\", \"you\", \"for\", \"your\", \"message\", \",\"], it produces \"Thank you for your message,\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627887bb82b4d005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:36:36.745750Z",
     "start_time": "2025-04-24T18:36:36.740764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence 1: Thank your message to show our words to the doctor, as his next contract checking, to all of us.\n",
      "Reconstructed Sentence 1: Thank you for your message to show our words to the doctor, during his next contract review, to all of us.\n",
      "\n",
      "Original Sentence 2: Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets.\n",
      "Reconstructed Sentence 2: Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets.\n"
     ]
    }
   ],
   "source": [
    "def process_sentence(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    tagged = tag(tokens)\n",
    "    transformed = apply_rules(tagged)\n",
    "    reconstructed = reconstruct(transformed)\n",
    "    return reconstructed\n",
    "\n",
    "# Test the pipeline\n",
    "sentence1 = \"Thank your message to show our words to the doctor, as his next contract checking, to all of us.\"\n",
    "sentence2 = \"Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets.\"\n",
    "\n",
    "print(\"Original Sentence 1:\", sentence1)\n",
    "print(\"Reconstructed Sentence 1:\", process_sentence(sentence1))\n",
    "print(\"\\nOriginal Sentence 2:\", sentence2)\n",
    "print(\"Reconstructed Sentence 2:\", process_sentence(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9412d35a875ea",
   "metadata": {},
   "source": [
    "### **Expected Output**\n",
    "Original Sentence | Reconstructed Sentence\n",
    "---|---\n",
    "Thank your message to show our words to the doctor, as his next contract checking, to all of us. | Thank you for your message to show our words to the doctor during his next contract review, to all of us.\n",
    "Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets. | Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85192aac4b5ff42e",
   "metadata": {},
   "source": [
    "### **Analysis**:\n",
    "- Sentence 1: The reconstruction clarifies \"Thank your message\" to \"Thank you for your message\" and \"as his next contract checking\" to \"during his next contract review,\" making the purpose and timing clearer. The phrase \"to all of us\" remains unchanged, as it’s sufficiently clear.\n",
    "- Sentence 2: No transformations are applied, as the sentence is relatively clear. The coordination in \"with strong coffee and future targets\" could be clarified (e.g., \"while setting future targets\"), but we retain the original for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e23bbb0fd00f23",
   "metadata": {},
   "source": [
    "### **8. Addressing Ambiguities**\n",
    "Sentence 1 Ambiguities\n",
    "- \"Thank your message\": Likely a typo or non-standard phrasing, possibly meant as \"Thank you for your message.\" The rule corrects this to a standard expression of gratitude.\n",
    "- \"as his next contract checking\": Unclear modifier, possibly indicating timing or purpose. Rephrasing to \"during his next contract review\" suggests a temporal context and uses \"review\" as a clearer noun.\n",
    "- \"to all of us\": Could be ambiguous in attachment, but interpreted as benefiting the group, so left unchanged.\n",
    "\n",
    "Sentence 2 Ambiguities\n",
    "- \"with strong coffee and future targets\": Could imply celebrating with both coffee and targets or setting targets separately. The original is retained, as it’s reasonably clear, but could be rephrased for explicitness if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b1dd6ca745e57",
   "metadata": {},
   "source": [
    "### **9. Limitations**\n",
    "- Scalability: The dictionary-based POS tagging and specific rules are tailored to these sentences, limiting generalization.\n",
    "- Complexity: Without libraries, we omit advanced techniques like parsing trees or statistical disambiguation.\n",
    "- Ambiguity Resolution: Some ambiguities (e.g., attachment of \"to all of us\") rely on interpretation, which may not be definitive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967be39b597f143",
   "metadata": {},
   "source": [
    "### **10. Conclusion**\n",
    "This pipeline demonstrates how basic NLP techniques can clarify ambiguous sentences without external libraries. By tokenizing, tagging, applying custom rules, and reconstructing, we enhance readability while adhering to the task’s constraints. The markdown explanations provide insight into NLP concepts, making the notebook educational and practical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
