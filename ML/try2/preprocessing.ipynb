{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e80dcce",
   "metadata": {},
   "source": [
    "# Creating a Large Dataset for Ambiguity Detection\n",
    "\n",
    "In this notebook, we will create a large dataset of sentence pairs consisting of original non-ambiguous sentences and their ambiguous counterparts. This dataset will be used to train an RNN encoder/decoder model for sequence-to-sequence sentence reconstruction. The pipeline involves downloading a large dataset of email-like sentences (the EnronSent Corpus), processing it to extract sentences with 10-20 words, creating a vocabulary lookup table, and introducing ambiguities using the provided functions.\n",
    "\n",
    "The EnronSent Corpus is a cleaned version of the Enron Email Dataset, containing 96,106 messages across 45 plain text files, totaling 13.8 million words. It is suitable for linguistic analysis and matches the formal, email-like style of your target texts.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Download and Extract**: Download the EnronSent Corpus and extract the text files.\n",
    "2. **Sentence Selection**: Extract sentences with 10-20 words (excluding punctuation) and collect tokens, lemmas, and POS tags.\n",
    "3. **Vocabulary Creation**: Generate a vocabulary lookup table with token, lemma, and POS columns.\n",
    "4. **Ambiguity Introduction**: Apply lexical, structural, and referential ambiguity functions to create ambiguous counterparts.\n",
    "5. **Save Outputs**: Save the dataset and vocabulary lookup table as CSV files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138d5af",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries\n",
    "\n",
    "We need libraries for downloading the dataset, processing text, performing NLP tasks, and handling data. SpaCy is used for tokenization, lemmatization, POS tagging, and dependency parsing, while NLTK's WordNet is used for synonym lookup in the lexical ambiguity function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb0f007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\claza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\claza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import libraries\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from glob import glob\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912e412",
   "metadata": {},
   "source": [
    "## Step 2: Download and Extract the EnronSent Corpus\n",
    "\n",
    "We download the EnronSent Corpus from the University of California San Diego. The dataset is a 25MB tar.gz file containing 45 plain text files. After downloading, we extract it to a folder named 'enronsentv1'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"http://wstyler.ucsd.edu/files/enronsentv1.tar.gz\"\n",
    "filename = \"enronsentv1.tar.gz\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# Extract the tar.gz file\n",
    "with tarfile.open(filename, 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0078e3c5",
   "metadata": {},
   "source": [
    "## Step 3: Process Text Files to Select Sentences\n",
    "\n",
    "We process each text file to extract sentences with 10-20 words (excluding punctuation). For each selected sentence, we store the text, tokens, lemmas, and fine-grained POS tags (e.g., 'NN', 'VB') using SpaCy's `token.tag_` attribute, which matches the Penn Treebank tags required by your ambiguity functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f8be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b12d96d1bb841adae5a0461a891499e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The dataset is extracted to a folder named 'enronsentv1'# Download the dataset\n",
    "from glob import glob\n",
    "extracted_folder = \"enronsent\"\n",
    "text_files = glob(os.path.join(extracted_folder, \"enronsent??\"))\n",
    "# Function to process a single file\n",
    "def process_file(file_path, nlp, chunk_size=500_000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    sentences_data = []\n",
    "    for start in range(0, len(text), chunk_size):\n",
    "        chunk = text[start:start+chunk_size]\n",
    "        doc = nlp(chunk)\n",
    "        for sent in doc.sents:\n",
    "            tokens = [t.text for t in sent if not t.is_punct]\n",
    "            if 10 <= len(tokens) <= 20:\n",
    "                sentences_data.append({\n",
    "                    'text': sent.text,\n",
    "                    'tokens': tokens,\n",
    "                    'lemmas': [t.lemma_ for t in sent if not t.is_punct],\n",
    "                    'pos_tags': [t.tag_ for t in sent if not t.is_punct],\n",
    "                })\n",
    "    return sentences_data\n",
    "\n",
    "# Process all files\n",
    "selected_sentences = []\n",
    "for file in tqdm(text_files):\n",
    "    selected_sentences.extend(process_file(file, nlp))\n",
    "\n",
    "# save the selected sentences to a CSV file\n",
    "df = pd.DataFrame(selected_sentences)\n",
    "df.to_csv(\"enron_sentences.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a04a5",
   "metadata": {},
   "source": [
    "## Step 4: Create Vocabulary Lookup Table\n",
    "\n",
    "We extract all unique (token, lemma, pos) triples from the selected sentences to create a vocabulary lookup table. The table has columns `token`, `lemma`, and `pos`, matching the structure of your existing `vocab_lookup_original_texts.csv` for easy merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all (token, lemma, pos) triples\n",
    "vocab_data = []\n",
    "for sent in selected_sentences:\n",
    "    for token, lemma, pos in zip(sent['tokens'], sent['lemmas'], sent['pos_tags']):\n",
    "        vocab_data.append((token, lemma, pos))\n",
    "\n",
    "# Remove duplicates\n",
    "unique_vocab = list(set(vocab_data))\n",
    "\n",
    "# Create DataFrame\n",
    "vocab_df = pd.DataFrame(unique_vocab, columns=['token', 'lemma', 'pos'])\n",
    "\n",
    "# Save to CSV\n",
    "vocab_df.to_csv('data/new_vocab_lookup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ae56b",
   "metadata": {},
   "source": [
    "## Step 5: Create Vocabulary List for Ambiguity Introduction\n",
    "\n",
    "The `vocab_list` is a list of unique (word, pos) pairs used by the `introduce_lexical_ambiguity` function to find replacement words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa37f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab_list\n",
    "vocab_list = list(set((token, pos) for sent in selected_sentences for token, pos in zip(sent['tokens'], sent['pos_tags'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f961e1",
   "metadata": {},
   "source": [
    "## Step 6: Define Ambiguity Introduction Functions\n",
    "\n",
    "We define the provided functions for introducing lexical, structural, and referential ambiguities. These functions are used as-is to ensure consistency with your original approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086407c",
   "metadata": {},
   "source": [
    "## Step 6: Define Ambiguity Introduction Functions\n",
    "\n",
    "We define the provided functions for introducing lexical, structural, and referential ambiguities. These functions are used as-is to ensure consistency with your original approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6601e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Ambiguity\n",
    "def introduce_lexical_ambiguity(sentence, tokens, lemmas, pos_tags, vocab_list):\n",
    "    candidates = [(lemma, pos, idx) for idx, (lemma, pos) in enumerate(zip(lemmas, pos_tags)) \n",
    "                  if pos.startswith(('NN', 'VB', 'JJ'))]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    \n",
    "    orig_lemma, orig_pos, orig_idx = random.choice(candidates)\n",
    "    ambiguous_candidates = [w for w, p in vocab_list if p == orig_pos and w != orig_lemma]\n",
    "    ambiguous_candidates = [w for w in ambiguous_candidates \n",
    "                            if len(wn.synsets(w, pos=wn.NOUN if orig_pos.startswith('NN') else \n",
    "                                                    wn.VERB if orig_pos.startswith('VB') else wn.ADJ)) > 1]\n",
    "    replacement = random.choice(ambiguous_candidates) if ambiguous_candidates else None\n",
    "    if not replacement:\n",
    "        homonyms = [w for w, p in vocab_list if w == orig_lemma and p != orig_pos]\n",
    "        replacement = orig_lemma if homonyms else None\n",
    "    \n",
    "    if not replacement:\n",
    "        return None\n",
    "    \n",
    "    new_tokens = tokens.copy()\n",
    "    new_tokens[orig_idx] = replacement\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "# Structural Ambiguity\n",
    "def introduce_structural_ambiguity(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    # 1. Try to find an existing PP to reattach\n",
    "    pp_token = None\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'prep' and token.head.dep_ != 'ROOT':\n",
    "            pp_token = token\n",
    "            break\n",
    "\n",
    "    if pp_token:\n",
    "        # extract PP span text\n",
    "        pp_span = doc[pp_token.i : pp_token.i + len(list(pp_token.subtree))]\n",
    "        pp_text = pp_span.text\n",
    "        head = pp_token.head\n",
    "\n",
    "        tokens = [t.text for t in doc]\n",
    "        # noun-attached → move to verb/clause\n",
    "        if head.pos_ in {'NOUN', 'PROPN'}:\n",
    "            start, end = pp_token.i, pp_token.i + len(list(pp_token.subtree))\n",
    "            base = tokens[:start] + tokens[end:]\n",
    "            # insert before final punctuation\n",
    "            if base and base[-1] in {'.','?','!'}:\n",
    "                base.insert(-1, pp_text)\n",
    "            else:\n",
    "                base.append(pp_text)\n",
    "            return ' '.join(base)\n",
    "\n",
    "        # verb-attached → attach to the direct object\n",
    "        if head.pos_ == 'VERB':\n",
    "            dobj = next((c for c in head.children if c.dep_ == 'dobj'), None)\n",
    "            if dobj:\n",
    "                dobj_span = doc[dobj.i : dobj.i + len(list(dobj.subtree))]\n",
    "                out = []\n",
    "                for tok in doc:\n",
    "                    out.append(tok.text)\n",
    "                    if tok.i == dobj_span[-1].i:\n",
    "                        out.append(pp_text)\n",
    "                return ' '.join(out)\n",
    "\n",
    "    # —fallback: no usable PP found or no dobj—\n",
    "    # 2. Try to build a PP from an existing noun in the sentence\n",
    "    nouns = [t.text for t in doc if t.pos_ in {'NOUN','PROPN'}]\n",
    "    if nouns:\n",
    "        obj = random.choice(nouns)\n",
    "        return sentence.rstrip(' .?!') + f\" with {obj}.\"\n",
    "\n",
    "    # 3. Last-resort generic PP list\n",
    "    fallback_pps = [\n",
    "        \"with enthusiasm\",\n",
    "        \"on the table\",\n",
    "        \"in the room\",\n",
    "        \"by the window\",\n",
    "        \"for the first time\"\n",
    "    ]\n",
    "    choice = random.choice(fallback_pps)\n",
    "    return sentence.rstrip(' .?!') + f\" {choice}.\"\n",
    "\n",
    "# Referential Ambiguity\n",
    "def introduce_referential_ambiguity(sentence, tokens, pos_tags):\n",
    "    noun_indices = [i for i, pos in enumerate(pos_tags) if pos.startswith('NNP') or pos.startswith('NN')]\n",
    "    if len(noun_indices) < 2:\n",
    "        return None\n",
    "    \n",
    "    i, j = noun_indices[0], noun_indices[1]\n",
    "    noun1, noun2 = tokens[i], tokens[j]\n",
    "    \n",
    "    pronoun = \"it\"  # Default for abstract nouns in seeded sentences\n",
    "    if pos_tags[j] in ('NNS', 'NNPS'):\n",
    "        pronoun = \"they\"\n",
    "    elif noun2[0].isupper():\n",
    "        pronoun = \"he\"\n",
    "    \n",
    "    new_tokens = tokens.copy()\n",
    "    new_tokens[j] = pronoun\n",
    "    if j > 0 and pos_tags[j-1] in ('DT', 'PRP$'):\n",
    "        new_tokens[j-1] = ''\n",
    "    ambiguous_sentence = ' '.join([t for t in new_tokens if t])\n",
    "    return ambiguous_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107f5fa",
   "metadata": {},
   "source": [
    "## Step 7: Create Dataset with Ambiguous Sentences\n",
    "\n",
    "For each selected sentence, we apply the lexical, structural, and referential ambiguity functions. Each successful ambiguous sentence is paired with the original, along with the ambiguity type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ecb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ec4029d87b44c09b7319abfc6604bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating dataset:   0%|          | 0/326283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentence pairs written: 932354\n"
     ]
    }
   ],
   "source": [
    "output_file = 'data/final_dataset.csv'\n",
    "# Remove existing file if it exists so headers are written correctly\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "chunk_size = 100\n",
    "chunk = []\n",
    "total_pairs = 0\n",
    "\n",
    "for i, sent_data in enumerate(tqdm(selected_sentences, desc=\"Creating dataset\")):\n",
    "    original = sent_data['text']\n",
    "    tokens = sent_data['tokens']\n",
    "    lemmas = sent_data['lemmas']\n",
    "    pos_tags = sent_data['pos_tags']\n",
    "\n",
    "    # Lexical ambiguity\n",
    "    ambiguous_lex = introduce_lexical_ambiguity(original, tokens, lemmas, pos_tags, vocab_list)\n",
    "    if ambiguous_lex and ambiguous_lex != original:\n",
    "        chunk.append({'original': original, 'ambiguous': ambiguous_lex, 'type': 'lexical'})\n",
    "        total_pairs += 1\n",
    "\n",
    "    # Structural ambiguity\n",
    "    ambiguous_struct = introduce_structural_ambiguity(original)\n",
    "    if ambiguous_struct and ambiguous_struct != original:\n",
    "        chunk.append({'original': original, 'ambiguous': ambiguous_struct, 'type': 'structural'})\n",
    "        total_pairs += 1\n",
    "\n",
    "    # Referential ambiguity\n",
    "    ambiguous_ref = introduce_referential_ambiguity(original, tokens, pos_tags)\n",
    "    if ambiguous_ref and ambiguous_ref != original:\n",
    "        chunk.append({'original': original, 'ambiguous': ambiguous_ref, 'type': 'referential'})\n",
    "        total_pairs += 1\n",
    "\n",
    "    # Every chunk_size records, write to disk and clear memory\n",
    "    if len(chunk) >= chunk_size:\n",
    "        df_chunk = pd.DataFrame(chunk)\n",
    "        header = not os.path.exists(output_file)\n",
    "        df_chunk.to_csv(output_file, mode='a', index=False, header=header)\n",
    "        chunk = []\n",
    "\n",
    "# Write any remaining records\n",
    "if chunk:\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "    header = not os.path.exists(output_file)\n",
    "    df_chunk.to_csv(output_file, mode='a', index=False, header=header)\n",
    "\n",
    "print(f\"Total number of sentence pairs written: {total_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe75da6",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Vocabulary Lookup Table**: The `new_vocab_lookup.csv` file contains columns `token`, `lemma`, and `pos` (fine-grained POS tags like 'NN', 'VB'). This matches the structure of your existing `vocab_lookup_original_texts.csv`.\n",
    "- **Dataset**: The `new_final_dataset.csv` file contains columns `original`, `ambiguous`, and `type` (indicating the type of ambiguity introduced).\n",
    "- **Special Tokens**: The special tokens (`<PAD>`, `<SOS>`, `<EOS>`, `<UNK>`) are not included in this pipeline as they are typically handled during model training.\n",
    "- **Ambiguity Functions**: The provided functions are used as-is, ensuring consistency with your original approach.\n",
    "- **Performance**: Processing the entire corpus may be memory-intensive. If issues arise, consider processing files in smaller batches or sampling sentences.\n",
    "\n",
    "This pipeline can be executed in a Jupyter Notebook to generate the required dataset and vocabulary lookup table. Ensure all libraries are installed and run the cells in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28203391",
   "metadata": {},
   "source": [
    "##  Build Wait-K Prefix Dataset\n",
    "We start from original parallel pairs \\((\\mathbf{x}, \\mathbf{y}) = (x_{1:L_x}, y_{1:L_y})\\) and apply a Wait-\\(K\\) strategy.  For each target position \\(t=1,\\dots,L_y\\), define the number of source tokens seen so far as  \n",
    "$$\n",
    "r(t) \\;=\\;\\min\\bigl(K + (t-1),\\,L_x\\bigr).\n",
    "$$  \n",
    "We then create prefix-to-next-word examples of the form  \n",
    "$$\n",
    "\\bigl(x_{1:r(t)},\\,y_{t-1}\\bigr)\\;\\mapsto\\;y_t,\n",
    "$$  \n",
    "where \\(y_0\\) is the special `<sos>` token.  In practice this yields up to \\(L_y\\) examples per sentence pair.  Finally, we save all examples to `final_dataset_2.csv` with columns:\n",
    "\n",
    "- `source_prefix` = $x_{1:r(t)}$\n",
    "- `prev_target`   = $y_{t-1}$\n",
    "- `target_word`   = $y_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4239b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11724190 prefix examples to final_dataset_2.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Load your original parallel corpus CSV\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/final_dataset.csv')\n",
    "\n",
    "K = 3  # choose your latency budget\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    src_tokens = row['ambiguous'].split()\n",
    "    tgt_tokens = row['original'].split()\n",
    "    Lx, Ly = len(src_tokens), len(tgt_tokens)\n",
    "    for t in range(1, Ly + 1):\n",
    "        r = min(K + (t - 1), Lx)\n",
    "        prev_tok = tgt_tokens[t - 2] if t > 1 else '<sos>'\n",
    "        rows.append({\n",
    "            'source_prefix': ' '.join(src_tokens[:r]),\n",
    "            'prev_target': prev_tok,\n",
    "            'target_word': tgt_tokens[t - 1]\n",
    "        })\n",
    "\n",
    "new_df = pd.DataFrame(rows)\n",
    "new_df.to_csv('data/final_dataset_2.csv', index=False)\n",
    "print(f\"Saved {len(new_df)} prefix examples to final_dataset_2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
