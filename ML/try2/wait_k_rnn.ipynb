{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63268b12",
   "metadata": {},
   "source": [
    "# RNN-based Seq2Seq Model for Sentence Disambiguation\n",
    "## Introduction and Problem Overview\n",
    "Sentence disambiguation can be framed as a **paraphrase generation** task: we take an ambiguous sentence and generate a rephrased version that resolves ambiguities (lexical, structural, referential) while preserving the original meaning.\n",
    "\n",
    "We will implement a **recurrent neural network (RNN)** based encoder-decoder (seq2seq) model in PyTorch, largely from scratch (no high-level seq2seq libraries). Our design emphasizes:\n",
    "- **Minimal external dependencies:** We'll use only PyTorch and Python standard libraries, writing our own tokenizer, data pipeline, and network modules.\n",
    "- **Flexibility in rephrasing:** The model is not constrained to copy input tokens exactly; it can learn to produce different words or reorder phrases to resolve ambiguity.\n",
    "- **Expressivity for disambiguation:** We use an architecture (with choices like LSTM units and an attention mechanism) capable of capturing context and meaning needed to handle lexical choice, structural reordering, and pronoun resolution.\n",
    "- **Scientific rigor:** Each design choice (embedding size, hidden layer type/size, attention, etc.) is justified with reference to established research or best practices.\n",
    "- **Device adaptability:** The implementation will automatically use GPU if available, falling back to CPU gracefully.\n",
    "- **Consistency with provided preprocessing:** We will follow the same tokenization and vocabulary construction approach as in the provided `preprocessing.ipynb/vocab_lookup`, ensuring our data pipeline (e.g. handling of special tokens, casing, and underscores) matches the intended setup.\n",
    "\n",
    "\n",
    "#### for detailed information on the architecture refer to `rnn.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b5256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built merged vocab: 89376 tokens (incl. specials)\n",
      "Prepended specials: ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
      "Total unique tokens from CSV: 89372\n",
      "New vocab size = 89376\n",
      "Train examples: 10551771, Val examples: 1172419 on device cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tokenize(sentence):\n",
    "    # Split by any whitespace or punctuation, keeping punctuation as separate tokens.\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence, flags=re.UNICODE)\n",
    "    return [tok.lower() for tok in tokens]\n",
    "\n",
    "\n",
    "# --- 1) Load & parse original‐texts vocab (has 'tokens', 'frequency', 'examples') ---\n",
    "orig_df = pd.read_csv('data/vocab_lookup_original_texts.csv')\n",
    "raw_tokens = set()\n",
    "for cell in orig_df['tokens'].dropna():\n",
    "    try:\n",
    "        lst = ast.literal_eval(cell)\n",
    "        if isinstance(lst, list):\n",
    "            raw_tokens.update(tok.lower() for tok in lst)\n",
    "    except Exception:\n",
    "        pass  # skip malformed\n",
    "\n",
    "# --- 2) Load & parse new dataset vocab (has 'token', 'lemma', 'pos') ---\n",
    "new_df = pd.read_csv('data/new_vocab_lookup.csv')\n",
    "raw_tokens.update(tok.lower() for tok in new_df['token'].dropna().astype(str))\n",
    "\n",
    "# --- 3) Define special tokens & ensure no duplicates ---\n",
    "specials = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "for s in specials:\n",
    "    raw_tokens.discard(s)\n",
    "\n",
    "# --- 4) Build final ordered vocab & mappings ---\n",
    "all_tokens  = specials + sorted(raw_tokens)\n",
    "word2index  = {tok: idx for idx, tok in enumerate(all_tokens)}\n",
    "index2word  = {idx: tok for tok, idx in word2index.items()}\n",
    "vocab_size  = len(all_tokens)\n",
    "\n",
    "print(f\"Built merged vocab: {vocab_size} tokens (incl. specials)\")\n",
    "print(f\"Prepended specials: {specials}\")\n",
    "print(f\"Total unique tokens from CSV: {len(raw_tokens)}\")\n",
    "print(f\"New vocab size = {vocab_size}\")\n",
    "\n",
    "# --- 5) Simple tokenizer: keeps alphanumeric + underscore tokens together,\n",
    "#    splits off punctuation.\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
    "    return [t.lower() for t in tokens]\n",
    "# --- 6) Define Dataset class ---\n",
    "class FinalDataset(Dataset):\n",
    "    def __init__(self, csv_path, w2i, tokenizer):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.src_prefixes = df['source_prefix'].astype(str).tolist()\n",
    "        self.prev_tgts    = df['prev_target'].astype(str).tolist()\n",
    "        self.next_tgts    = df['target_word'].astype(str).tolist()\n",
    "        self.w2i = w2i\n",
    "        self.tokenize = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_prefixes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tokenize(self.src_prefixes[idx])\n",
    "        prev = self.prev_tgts[idx]\n",
    "        nxt  = self.next_tgts[idx]\n",
    "        src_ids  = [ self.w2i.get(t, self.w2i['<UNK>']) for t in src ]\n",
    "        prev_id  = self.w2i.get(prev, self.w2i['<UNK>'])\n",
    "        next_id  = self.w2i.get(nxt,  self.w2i['<UNK>'])\n",
    "        return torch.tensor(src_ids, dtype=torch.long), prev_id, next_id\n",
    "# --- 7) Define collate_fn for padding sequences ---\n",
    "def collate_fn(batch):\n",
    "    src_seqs, prev_ids, next_ids = zip(*batch)\n",
    "    src_lens = [len(s) for s in src_seqs]\n",
    "    src_pad  = pad_sequence(src_seqs, batch_first=True, padding_value=word2index['<PAD>'])\n",
    "    return (\n",
    "        src_pad,\n",
    "        torch.tensor(src_lens, dtype=torch.long),\n",
    "        torch.tensor(prev_ids, dtype=torch.long),\n",
    "        torch.tensor(next_ids, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "batch_size = 128\n",
    "K = 3  \n",
    "\n",
    "# 8) Load & split\n",
    "dataset2 = FinalDataset('data/final_dataset_2.csv', word2index, tokenize)\n",
    "val_size = int(0.1 * len(dataset2))\n",
    "train_size = len(dataset2) - val_size\n",
    "train_ds, val_ds = random_split(dataset2, [train_size, val_size],\n",
    "                                 generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader2 = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader2   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train examples: {len(train_ds)}, Val examples: {len(val_ds)} on device {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 9) Model definition ---\n",
    "# --- 1) Device setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2index['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)                    # (B, L_in, E)\n",
    "        outputs, hidden = self.lstm(emb)           # outputs=(B,L_in,H), hidden=(h_n,c_n)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2index['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.out  = nn.Linear(hidden_size*2, vocab_size)\n",
    "\n",
    "    def forward_step(self, prev_tok, hidden, enc_outputs):\n",
    "        emb = self.embedding(prev_tok).unsqueeze(1)     # (B,1,E)\n",
    "        out, hidden = self.lstm(emb, hidden)            # out=(B,1,H)\n",
    "        # dot-product attention\n",
    "        scores = torch.bmm(out, enc_outputs.transpose(1,2))  # (B,1,L_in)\n",
    "        attn  = torch.softmax(scores, dim=2)                # (B,1,L_in)\n",
    "        ctx   = torch.bmm(attn, enc_outputs).squeeze(1)     # (B,H)\n",
    "        out_t = out.squeeze(1)                              # (B,H)\n",
    "        cat   = torch.cat([out_t, ctx], dim=1)              # (B,2H)\n",
    "        logits= self.out(cat)                               # (B,V)\n",
    "        return logits, hidden, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3260316",
   "metadata": {},
   "source": [
    "We train the same encoder–decoder with attention on these prefix examples.  For each example \\(i\\) with source prefix \\(x_{1:r_i}\\) and previous target \\(y_{t-1}^{(i)}\\):\n",
    "\n",
    "1. **Encode prefix**  \n",
    "   $$\n",
    "     (h_{1:r_i}, c_{1:r_i})\n",
    "     = \\mathrm{Encoder}(x_{1:r_i}).\n",
    "   $$\n",
    "\n",
    "2. **Single-step decode**  \n",
    "   $$\n",
    "     \\tilde{h}_t\n",
    "     = \\mathrm{DecoderStep}\\bigl(y_{t-1},\\,h_{r_i},\\,c_{r_i},\\,h_{1:r_i}\\bigr).\n",
    "   $$\n",
    "\n",
    "3. **Dot-product attention**  \n",
    "   $$\n",
    "     \\alpha_j\n",
    "     = \\frac{\\exp(\\tilde{h}_t^\\top h_j)}\n",
    "            {\\sum_{k=1}^{r_i}\\exp(\\tilde{h}_t^\\top h_k)},\\quad\n",
    "     c_t = \\sum_{j=1}^{r_i}\\alpha_j\\,h_j.\n",
    "   $$\n",
    "\n",
    "4. **Prediction & loss**  \n",
    "   $$\n",
    "     \\hat y_t\n",
    "     = \\arg\\max\\mathrm{Softmax}\\bigl(W[\\tilde{h}_t; c_t]+b\\bigr),\\quad\n",
    "     \\mathcal{L}\n",
    "     = -\\sum_i \\log p_\\theta\\bigl(y_t^{(i)}\\mid x_{1:r_i}^{(i)},\\,y_{<t}^{(i)}\\bigr).\n",
    "   $$\n",
    "\n",
    "We backpropagate this cross-entropy loss and update all model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42be5ede436414fac6e8e0ffc1f8f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/82436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     33\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(params, max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     total_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     36\u001b[0m avg_train \u001b[38;5;241m=\u001b[39m total_train \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader2)\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[1;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\optim\\adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    237\u001b[0m         group,\n\u001b[0;32m    238\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         state_steps,\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\optim\\adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\optim\\adam.py:527\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 527\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import torch, torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size, hidden_size, num_layers = 512, 512, 2\n",
    "num_epochs, lr = 20, 1e-3\n",
    "checkpoint_dir = '/kaggle/input/wait-k-rnn-checkpoint/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Model, optimizer, criterion\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "params    = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2index['<PAD>'])\n",
    "\n",
    "# === Resume Logic ===\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth'))\n",
    "if checkpoint_files:\n",
    "    latest_ckpt = max(\n",
    "        checkpoint_files,\n",
    "        key=lambda x: int(os.path.splitext(x)[0].split('_')[-1])\n",
    "    )\n",
    "    ckpt = torch.load(latest_ckpt, map_location=device)\n",
    "    encoder.load_state_dict(ckpt['encoder_state_dict'])\n",
    "    decoder.load_state_dict(ckpt['decoder_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {ckpt['epoch']} → starting at {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    print(\"No checkpoint found, starting from scratch\")\n",
    "# === Training Loop ===\n",
    "print(f\"resuming from epoch:{start_epoch}\")\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    encoder.train(); decoder.train()\n",
    "    total_train = 0.\n",
    "    for src_pad, src_lens, prev_ids, tgt_ids in tqdm(train_loader2, desc=f\"Epoch {epoch} Training\"):\n",
    "        src_pad, src_lens = src_pad.to(device), src_lens.to(device)\n",
    "        prev_ids, tgt_ids = prev_ids.to(device), tgt_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        enc_out, enc_hidden = encoder(src_pad)\n",
    "        logits, dec_hidden, _ = decoder.forward_step(prev_ids, enc_hidden, enc_out)\n",
    "        loss = criterion(logits, tgt_ids)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_train += loss.item()\n",
    "\n",
    "    avg_train = total_train / len(train_loader2)\n",
    "\n",
    "    # Validation\n",
    "    encoder.eval(); decoder.eval()\n",
    "    total_val = 0.\n",
    "    with torch.no_grad():\n",
    "        for src_pad, src_lens, prev_ids, tgt_ids in val_loader2:\n",
    "            src_pad, src_lens = src_pad.to(device), src_lens.to(device)\n",
    "            prev_ids, tgt_ids = prev_ids.to(device), tgt_ids.to(device)\n",
    "            enc_out, enc_hidden = encoder(src_pad)\n",
    "            logits, _, _      = decoder.forward_step(prev_ids, enc_hidden, enc_out)\n",
    "            total_val += criterion(logits, tgt_ids).item()\n",
    "    avg_val = total_val / len(val_loader2)\n",
    "\n",
    "    print(f\"Epoch {epoch} → Train: {avg_train:.4f}, Val: {avg_val:.4f}\")\n",
    "\n",
    "    # Checkpoint\n",
    "    ckpt_path = os.path.join('/kaggle/working', f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "# Final model saves\n",
    "os.makedirs('models', exist_ok=True)\n",
    "torch.save(encoder.state_dict(), 'models/encoder_2.pth')\n",
    "torch.save(decoder.state_dict(), 'models/decoder_2.pth')\n",
    "print(\"Saved final models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf05feb",
   "metadata": {},
   "source": [
    "At test time we generate one token at a time under the same Wait-$K$ schedule:\n",
    "\n",
    "- Initialize $t=1$, $r(1)=\\min(K, L_x)$, and $\\hat y_0=\\texttt{<sos>}$.  \n",
    "- Repeat until $\\hat y_{t-1}=\\texttt{<eos>}$ or $t>\\text{max\\_len}$:\n",
    "\n",
    "  1. **Encode prefix**  \n",
    "     $$\n",
    "       (h_{1:r(t)}, c_{1:r(t)})\n",
    "       = \\mathrm{Encoder}(x_{1:r(t)}).\n",
    "     $$\n",
    "\n",
    "  2. **Decode one step**  \n",
    "     $$\n",
    "       \\tilde{h}_t\n",
    "       = \\mathrm{DecoderStep}\\bigl(\\hat y_{t-1},\\,h_{r(t)},\\,c_{r(t)},\\,h_{1:r(t)}\\bigr).\n",
    "     $$\n",
    "\n",
    "  3. **Attention & predict**  \n",
    "     $$\n",
    "       \\alpha_j = \\frac{\\exp(\\tilde{h}_t^\\top h_j)}\n",
    "                       {\\sum_{k=1}^{r(t)}\\exp(\\tilde{h}_t^\\top h_k)},\\quad\n",
    "       c_t = \\sum_{j=1}^{r(t)}\\alpha_j\\,h_j,\n",
    "     $$\n",
    "     $$\n",
    "       \\hat y_t = \\arg\\max\\mathrm{Softmax}\\bigl(W[\\tilde{h}_t; c_t]+b\\bigr).\n",
    "     $$\n",
    "\n",
    "  4. **Advance**  \n",
    "     $t \\leftarrow t+1$ and  \n",
    "     $r(t)=\\min(K+(t-1),L_x)$.\n",
    "\n",
    "The output sequence $\\hat y_{1:T}$ is generated under the desired latency constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9271daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberts ownner confirmatiopn rails continental= deft > client overnighting in our context 3pm to celebrate it with all safe with all safe in\n"
     ]
    }
   ],
   "source": [
    "special_tokens = {'<PAD>','<SOS>','<EOS>','<UNK>'}\n",
    "# load the encoder and decoder models from the models directory\n",
    "# 1) Hyperparameters (reuse or override)\n",
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "encoder.load_state_dict(torch.load('models/encoder_2.pth', map_location=device))\n",
    "decoder.load_state_dict(torch.load('models/decoder_2.pth', map_location=device))\n",
    "\n",
    "def translate_wait_k(src_sentence, K, max_len=50):\n",
    "    encoder.eval(); decoder.eval()\n",
    "    src_toks = tokenize(src_sentence)\n",
    "    src_ids  = [ word2index.get(t, word2index['<UNK>']) for t in src_toks ]\n",
    "    outputs = []\n",
    "    t = 1\n",
    "    while True:\n",
    "        r = min(K + (t-1), len(src_ids))\n",
    "        inp = torch.tensor(src_ids[:r], device=device).unsqueeze(0)\n",
    "        enc_out, enc_hidden = encoder(inp)\n",
    "        prev_id = word2index['<SOS>'] if t == 1 else outputs[-1]\n",
    "        logits, dec_hidden, attn = decoder.forward_step(\n",
    "            torch.tensor([prev_id], device=device), enc_hidden, enc_out\n",
    "        )\n",
    "        # mask specials\n",
    "        for sp in special_tokens - {'<EOS>'}:\n",
    "            logits[:, word2index[sp]] = -1e9\n",
    "        next_id = logits.argmax(dim=1).item()\n",
    "        if next_id == word2index['<EOS>'] or t >= max_len:\n",
    "            break\n",
    "        tok = index2word[next_id]\n",
    "        # copy from source if UNK\n",
    "        if tok == '<UNK>':\n",
    "            a = attn.squeeze(0).squeeze(0)\n",
    "            src_pos = a.argmax().item()\n",
    "            tok = src_toks[src_pos]\n",
    "            next_id = word2index.get(tok, next_id)\n",
    "        if tok not in special_tokens:\n",
    "            outputs.append(next_id)\n",
    "        else:\n",
    "            outputs.append(next_id)\n",
    "        t += 1\n",
    "\n",
    "    return \" \".join(index2word[i] for i in outputs)\n",
    "\n",
    "# Sample usage:\n",
    "sentence = \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives.\"\n",
    "print(translate_wait_k(sentence, K=1, max_len=len(sentence.split())+3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
