{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3848c15",
   "metadata": {},
   "source": [
    "# RNN-based Seq2Seq Model for Sentence Disambiguation\n",
    "## Introduction and Problem Overview\n",
    "Sentence disambiguation can be framed as a **paraphrase generation** task: we take an ambiguous sentence and generate a rephrased version that resolves ambiguities (lexical, structural, referential) while preserving the original meaning.\n",
    "\n",
    "We will implement a **recurrent neural network (RNN)** based encoder-decoder (seq2seq) model in PyTorch, largely from scratch (no high-level seq2seq libraries). Our design emphasizes:\n",
    "- **Minimal external dependencies:** We'll use only PyTorch and Python standard libraries, writing our own tokenizer, data pipeline, and network modules.\n",
    "- **Flexibility in rephrasing:** The model is not constrained to copy input tokens exactly; it can learn to produce different words or reorder phrases to resolve ambiguity.\n",
    "- **Expressivity for disambiguation:** We use an architecture (with choices like LSTM units and an attention mechanism) capable of capturing context and meaning needed to handle lexical choice, structural reordering, and pronoun resolution.\n",
    "- **Scientific rigor:** Each design choice (embedding size, hidden layer type/size, attention, etc.) is justified with reference to established research or best practices.\n",
    "- **Device adaptability:** The implementation will automatically use GPU if available, falling back to CPU gracefully.\n",
    "- **Consistency with provided preprocessing:** We will follow the same tokenization and vocabulary construction approach as in the provided `preprocessing.ipynb/vocab_lookup`, ensuring our data pipeline (e.g. handling of special tokens, casing, and underscores) matches the intended setup.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b33f87",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Vocabulary\n",
    "**Tokenization:** We implement a custom tokenizer to split sentences into tokens. This involves splitting on whitespace and punctuation while preserving special token formatting from the dataset (e.g. the dataset uses underscores to combine multi-word units like \"join_forces\" or \"reniform_leaf\"). We assume the provided preprocessing already handled such cases, so our tokenizer will treat any sequence of alphanumeric characters (including `_`) as a token, and separate punctuation as individual tokens. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed29749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(sentence):\n",
    "    # Split by any whitespace or punctuation, keeping punctuation as separate tokens.\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence, flags=re.UNICODE)\n",
    "    return [tok.lower() for tok in tokens]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0afb0b",
   "metadata": {},
   "source": [
    "**Vocabulary Construction:** Using the tokenized data, we build a vocabulary mapping to integers. We include special tokens for padding and sequence markers:\n",
    "- `<PAD>` for padding shorter sequences in a batch\n",
    "- `<SOS>` (start-of-sequence) to signify the beginning of a target sentence for the decoder\n",
    "- `<EOS>` (end-of-sequence) to mark sentence end\n",
    "- `<UNK>` for any rare or out-of-vocabulary token (if applicable)\n",
    "We assign each unique token an index. The `vocab_lookup` provided likely contains such mappings; we either load it or reconstruct it identically by iterating over all tokenized sentences. \n",
    "This ensures the vocabulary is consistent and covers both input and output sentences. The dataset's use of underscores (e.g. \"northrop_osteoblastoma\" might be two tokens \"northrop\" and \"osteoblastoma\" or one token if combined) is preserved by our tokenizer, so our vocab will treat them as in the original preprocessing. We also ensure that any token that was considered a single unit in `vocab_lookup `remains so here.\n",
    "\n",
    "**Sequence Preparation:** For model training, each ambiguous sentence (tokenized) will be encoded as a sequence of input token indices, and its corresponding original sentence will be encoded as output indices, with `<SOS>` prepended and `<EOS>` appended to the target. We will likely pad sequences to the same length in batches. The data pipeline yields `(input_tensor, target_tensor)` pairs for training. This is analogous to how one would prepare data for a translation or paraphrase model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d977a43",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Model Architecture\n",
    "Architecture of a sequence-to-sequence model with an RNN encoder (blue) and decoder (green) with an attention mechanism (illustrated by the connections to the \"Attention Mechanism\" box). The encoder processes the input sequence into hidden states $h_{i}$, and the decoder uses those (via an attention-weighted context vector) to generate the output sequence one token at a time.\n",
    "Our model follows the classic encoder-decoder paradigm. The encoder RNN reads the input (ambiguous sentence) and produces a sequence of hidden states (and a final summarized state). The decoder RNN then generates the output (disambiguated sentence) token by token, using the encoder’s context. We incorporate an attention mechanism to allow the decoder to focus on relevant parts of the input at each generation step, which is crucial for handling longer or complex sentences where a simple fixed-length context vector would be insufficient. Below, we describe each component and justify our design choices:\n",
    "### Encoder RNN\n",
    "The encoder is a recurrent neural network that processes the input token sequence and encodes its meaning into a sequence of hidden states. We choose a Long Short-Term Memory (LSTM) network for the encoder (as opposed to a simple RNN or GRU) because LSTMs are known to capture long-term dependencies and avoid vanishing gradient issues. Ambiguities like referential pronouns often require remembering context from earlier in the sentence, which LSTM’s memory cell is well-suited for. (A Gated Recurrent Unit (GRU) is a viable alternative with a simpler architecture and fewer parameters, often performing similarly, but we opt for LSTM for maximum expressivity given the subtlety of disambiguation tasks.)\n",
    "\n",
    "**Encoder structure:** Each input word index is first mapped to a trainable embedding vector. We use an embedding size (dimensionality) of 300, which is a common choice balancing richness and efficiency (300-dimensional embeddings have been standard in many NLP tasks and are large enough to capture semantic nuances). The embeddings are learned from scratch (to minimize external dependencies; we could initialize with pretrained word embeddings for potentially better lexical generalization, but we stay within our self-contained setup). These embeddings are fed into the LSTM. We use a single LSTM layer with a hidden state size of 256 (this is a hyperparameter one might tune; values in the few hundreds are typical in seq2seq models). The hidden size controls the capacity of the context representation. We found 256 to be sufficient for the dataset size and complexity, while not overly large to hurt interpretability or require excessive training time. If needed, one could increase this to 512 for more capacity, but 256 already allows a large number of possible features in the context vector.\n",
    "\n",
    "\n",
    "Optionally, we could make the encoder bidirectional, meaning it consists of a forward LSTM (reading left-to-right) and a backward LSTM (right-to-left). A bidirectional encoder provides a more comprehensive context representation since it encodes information from both past and future tokens. This often improves performance in translation/paraphrasing tasks (as used by Bahdanau et al., 2015). However, for simplicity and interpretability, we will keep the encoder unidirectional in our implementation, noting that bidirectional encoding is a possible extension if we find the need to capture context from the right side (e.g., structural ambiguity might benefit from knowing the upcoming phrase). \n",
    "\n",
    "We also apply dropout (e.g. 10–20%) on the embeddings or between LSTM layers if we had multiple, to regularize and improve generalization. This prevents overfitting, especially since the training data may not be extremely large. Encoder output: The encoder returns two things: (1) the sequence of all intermediate hidden states $h_i$ (one per input token $i$), and (2) the final hidden state (and cell state) after processing the last token. If unidirectional, the final hidden state $h_f$ is a summary of the whole input sentence. If bidirectional, we would concatenate the final forward and backward states. These will be passed to the decoder. In an attention model, we will make use of the full sequence of encoder states as well, not just the final state.\n",
    "\n",
    "Below is a simplified implementation of the Encoder:\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        # If bidirectional: use self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        # and handle hidden state accordingly.\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        input_seq: Tensor of shape (batch_size, seq_length) with token indices.\n",
    "        \"\"\"\n",
    "        embed = self.embedding(input_seq)            # (batch, seq_length, embed_size)\n",
    "        outputs, hidden = self.lstm(embed)           # outputs: (batch, seq_length, hidden_size)\n",
    "        # 'hidden' is a tuple (h_n, c_n) each of shape (1, batch, hidden_size) for 1-layer LSTM.\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be633366",
   "metadata": {},
   "source": [
    "In this code, `batch_first=True` is used for convenience so that tensor shapes are as noted. We would move `input_seq` to the chosen device (CPU/GPU) before calling `encoder(input_seq)`. The encoder's `hidden` (which includes both hidden state and cell state for the LSTM) will be passed into the decoder to initialize its state.\n",
    "\n",
    "## Decoder RNN with Attention\n",
    "The decoder is another RNN (LSTM) that generates the output sequence one token at a time in an autoregressive fashion. At each time step, the decoder takes as input the embedding of the previous output token (starting with `<SOS>` for the first token) and updates its hidden state. It then produces a probability distribution over the next token in the output. We incorporate an attention mechanism so that at each step the decoder can attend to different parts of the encoder's output sequence, instead of relying only on a single context vector.\n",
    "\n",
    "**Decoder structure:** We use an LSTM for the decoder as well, with the same hidden size as the encoder (256) for simplicity. Using the same hidden dimensionality allows us to directly use the encoder's final hidden state to initialize the decoder's hidden state. We initialize the decoder's hidden and cell states with the encoder's final states (`decoder_hidden0 = encoder_hidden`), a common practice to give the decoder a starting context. This helps especially with referential ambiguity: for example, the encoder’s final hidden state may encode the presence of a specific entity that the decoder can immediately use to start generating the proper noun instead of a pronoun.\n",
    "\n",
    "At each decoding step $t$, the decoder LSTM takes the previous token (at $t-1$) as input (via its embedding) and updates its hidden state $s_t$. Without attention, one could use $s_t$ to predict the next word. However, with attention, we first compute a context vector $c_t$ as a weighted sum of all encoder hidden states ${h_0, h_1, ..., h_{T}}$. The weights come from an alignment model that scores how well each encoder state $h_i$ matches the decoder state $s_t$. We use the Bahdanau attention (additive attention) mechanism: the alignment score $e_{ti} = \\text{score}(s_t, h_i)$ is computed by a small feed-forward network (with learned parameters) that takes $s_t$ and $h_i$ and outputs a score (a single scalar). These scores are normalized with softmax to produce attention weights $\\alpha_{ti}$ that sum to 1. Intuitively, $s_t$ (the decoder's current state, which encapsulates what has been generated so far and what it is about to generate) will attend more to those $h_i$ that are relevant to producing the next word. For instance, if the decoder is about to output the disambiguated noun corresponding to a prior pronoun, the attention mechanism should assign higher weight to the encoder states around that pronoun's antecedent or contextual clues in the input.\n",
    "\n",
    "For efficiency and simplicity, we might implement Luong's dot-product attention (a specific case of attention) instead of a separate feed-forward network. In dot-product attention, the score is $e_{ti} = s_t^\\top h_i$ (assuming $s_t$ and $h_i$ are vectors of the same dimension). This avoids extra parameters and often works well if the hidden size is not too large. We will use this dot-product approach in code below, as our encoder and decoder hidden sizes match.\n",
    "\n",
    "Once we have the attention weights $\\alpha_{ti}$, we compute the context vector as $c_t = \\sum_i \\alpha_{ti} h_i$, the weighted sum of encoder outputs. The context vector $c_t$ is essentially the part of the input sentence the model is focusing on at step $t$. We then combine $c_t$ with the decoder's state $s_t$ to inform the next word prediction. A common combination is to concatenate $c_t$ and $s_t$ (or the output of the LSTM at that step) and feed through a linear layer to produce the vocabulary logits. This linear layer (plus softmax) is the decoder's output projection, mapping the combined vector to a probability distribution over the output vocabulary.\n",
    "\n",
    "**Decoder implementation:** Here is the decoder with attention integrated:\n",
    "```python\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        # For attention: no extra parameters if using dot-product\n",
    "        # If using additive attention, define layers like:\n",
    "        # self.attn = nn.Linear(hidden_size * 2, hidden_size) etc.\n",
    "        self.out = nn.Linear(hidden_size * 2, vocab_size)  # combines decoder hidden and context\n",
    "    def forward_step(self, prev_token, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Run a single decoder step (for one token).\n",
    "        prev_token: Tensor of shape (batch,) with the previous token index.\n",
    "        hidden: (h, c) tuple of decoder hidden state(s), each shape (1, batch, hidden_size).\n",
    "        encoder_outputs: Tensor (batch, seq_len, hidden_size) from the encoder.\n",
    "        \"\"\"\n",
    "        # Embed the previous token\n",
    "        emb = self.embedding(prev_token).unsqueeze(1)      # (batch, 1, embed_size)\n",
    "        # One step of LSTM\n",
    "        output, hidden = self.lstm(emb, hidden)            # output: (batch, 1, hidden_size)\n",
    "        dec_state = output  # this is s_t (the output state at current step)\n",
    "        # Attention: compute scores for each encoder output\n",
    "        # Using dot-product attention: score_i = s_t · h_i for each encoder hidden h_i\n",
    "        # dec_state is (batch, 1, hidden), encoder_outputs is (batch, seq_len, hidden)\n",
    "        scores = torch.bmm(dec_state, encoder_outputs.transpose(1, 2))   # (batch, 1, seq_len)\n",
    "        attn_weights = torch.softmax(scores, dim=2)                      # (batch, 1, seq_len)\n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)               # (batch, 1, hidden)\n",
    "        # Concatenate context and decoder state\n",
    "        context = context.squeeze(1)   # (batch, hidden)\n",
    "        dec_state = dec_state.squeeze(1)   # (batch, hidden)\n",
    "        attn_combined = torch.cat([dec_state, context], dim=1)  # (batch, 2*hidden)\n",
    "        # Final output layer to predict next token\n",
    "        logits = self.out(attn_combined)    # (batch, vocab_size)\n",
    "        return logits, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68805a",
   "metadata": {},
   "source": [
    "In this `forward_step` method, we take `prev_token` (the last generated token, or `<SOS>` at start) and the current `hidden` state, and we return the logits for the next token, the updated hidden state, and the attention weights. We would call this step by step in a loop to generate a whole sequence. By structuring it this way, we have fine-grained control and can easily apply teacher forcing during training (by providing the actual next token as `prev_token`) or use the model's own prediction during inference. \n",
    "\n",
    "**Justification of design choices:**\n",
    "- The hidden state size is the same as encoder (256) so that dot-product attention is feasible (no dimension mismatch) and we can directly use encoder's hidden state to initialize the decoder. This is a common design in seq2seq models.\n",
    "- We opted for LSTM decoder for the same reasons as encoder – it can maintain context of what has been generated so far, which is important for fluency and correctness (e.g., not repeating words or ensuring grammatical agreement).\n",
    "- Attention is included because it **greatly improves the ability to handle longer sequences and structural transformations**. Without attention, the decoder would rely only on a single context vector (the last encoder hidden state), which might not encode sufficient detail, especially if the sentence is long or complex (as noted by Bahdanau et al. 2014). With attention, the decoder can dynamically focus on, say, the part of the input that needs disambiguation at the right time. For example, when generating the word \"lutjanus_blackfordi\" in the above example, the model can attend to where \"it\" appears in the input and the surrounding context that indicates what \"it\" refers to.\n",
    "- Our simple dot-product attention has no additional parameters, but one could use a learnable additive attention mechanism for potentially better performance. In practice, both dot and additive attention yield similar results; dot is faster when dimensions are large.\n",
    "- We **do not strictly copy input tokens**; the decoder’s output layer is a full vocabulary softmax. The model is free to output tokens that never appeared in the input. This is crucial for lexical disambiguation: the correct disambiguated word might differ from the input word (or be a more specific term), and for referential disambiguation where the output noun might not appear in the input at all (as with pronoun \"it\"). Our network must learn these transformations from the training data. In cases of lexical substitution where the input word and output word are synonyms, the model essentially learns to translate between them (e.g., mapping \"springer\" to \"northrop\" in the dataset for lexical ambiguity). By not using any copy mechanism or outputting input tokens by default, we allow syntactic and lexical variation as required.\n",
    "- However, we also realize that sometimes copying parts of the input is necessary (many words will remain the same between ambiguous and original sentences). Our model can still learn to copy implicitly by attending to a word and outputting the same word (since the word will be in the vocabulary). For example, in the ambiguous sentence, many words like \"Syrupy\" or \"apologize\" remain the same in the original – the model can output them identically by attending to them and the decoder learning an identity mapping for those contexts. We considered incorporating a specialized copy mechanism or pointer network, which is often used in seq2seq to handle out-of-vocabulary or to ensure important tokens are carried over. But given our vocabulary covers the needed words and the task is less about unknown proper nouns (the data seems to include the needed entities in vocabulary), a standard seq2seq with attention suffices. Simpler architecture is preferable here for interpretability.\n",
    "\n",
    "**Handling of Punctuation and Special Tokens:** The model treats punctuation as tokens (e.g., \"?\" or \".\" are in the vocab). We include `<EOS>` at the end of every target, and the decoder is trained to generate `<EOS>` when it finishes the sentence. At inference, the generation loop will stop when `<EOS>` is produced, preventing infinite sentences. Padding tokens `<PAD>` will appear in encoder inputs (for batch processing) and possibly in decoder target sequences. We will ensure the model doesn’t confuse `<PAD>` as real input by masking out pad positions in the attention computation (so that attention weights on `<PAD>` tokens are zero). In the above implementation, if we pad encoder outputs, the dot-product scores for pad positions might be low anyway if we initialize pad token embedding to zero or a learned vector – but to be safe, we can set very negative scores for padded positions or pre-mask the encoder_outputs by zeroing them out for pads. Similarly, when computing loss, we will ignore `<PAD>` in the target.\n",
    "\n",
    "**Device Management (CPU vs GPU)**\n",
    "Our implementation checks for CUDA availability and uses GPU if possible, otherwise defaults to CPU. For example:\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(460, embed_size=300, hidden_size=256).to(device)\n",
    "decoder = Decoder(460, embed_size=300, hidden_size=256).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2d129",
   "metadata": {},
   "source": [
    "All inputs and tensors are then moved to `device` before computations. We design the code such that this is the only change needed to switch hardware – PyTorch will handle the tensor operations on the chosen device. This way, one can train on GPU for speed, but still run the model on CPU if no GPU is present (with a slower runtime but identical results). Throughout training and inference code, we'll be careful to send data to `device`, e.g., `input_seq = input_seq.to(device)`.\n",
    "\n",
    "## Training Procedure and Justifications\n",
    "We train the encoder-decoder model on the parallel dataset of ambiguous and original sentences. The training objective is to maximize the probability of the correct disambiguated (original) sentence given the ambiguous input. This is typically done by minimizing the cross-entropy loss between the predicted token distribution and the true next token at each decoder time step.\n",
    "\n",
    "During training, we use teacher forcing, a strategy where we feed the actual ground-truth token as the next input to the decoder at each step (instead of using the decoder’s predicted token). Teacher forcing helps the model converge faster by providing correct context in the early stages of training, preventing error accumulation. Formally, at decoder step $t$, we already know the true token $y_t$ from the original sentence, so we input $y_t$ (actually `<SOS>` for $t=0$, then $y_1$ for next, etc.) rather than the model’s guess $\\hat{y}t$. The decoder still produces a distribution from which we compute loss for that step. We do this for each time step of the output sequence. If the output sentence has length $L$, the loss is $\\frac{1}{L}\\sum{t=1}^{L} -\\log P(y_t \\mid y_{<t}, \\text{input})$ (averaged or summed and then averaged per batch). \n",
    "\n",
    "As training progresses, we may introduce **scheduled sampling** (gradually reducing the teacher forcing rate) to let the model experience its own predictions as inputs, improving robustness. For simplicity, we can start with teacher forcing 100% of the time, and later on use a probability (e.g. 0.9 decreasing to 0.5) of using the true token vs. the model's token.\n",
    "\n",
    "We use the **Adam optimizeR** (a widely used adaptive optimizer) with a moderate learning rate (e.g. 0.001). Adam is suitable for seq2seq models and often converges faster than vanilla SGD or momentum for such tasks. We also may apply **gradient clipping** (e.g. clip norm to 5) to prevent exploding gradients common in RNN training. \n",
    "\n",
    "The training loop will look roughly like this\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize models and optimizer\n",
    "encoder = Encoder(vocab_size, embed_size=300, hidden_size=256).to(device)\n",
    "decoder = Decoder(vocab_size, embed_size=300, hidden_size=256).to(device)\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2index[\"<PAD>\"])  # ignore padding in loss\n",
    "\n",
    "num_epochs = 20\n",
    "teacher_force_ratio = 1.0  # start with full teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train(); decoder.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:  # assuming we have a DataLoader for training data\n",
    "        inputs, targets = batch   # inputs: (batch, seq_len_input), targets: (batch, seq_len_target)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Encode input sequence\n",
    "        encoder_outputs, encoder_hidden = encoder(inputs)\n",
    "        # Initialize decoder hidden state as encoder's final hidden\n",
    "        decoder_hidden = encoder_hidden\n",
    "        batch_size = inputs.size(0)\n",
    "        # The first input to decoder is the <SOS> token for each sequence in the batch\n",
    "        decoder_input = torch.full((batch_size,), word2index[\"<SOS>\"], dtype=torch.long, device=device)\n",
    "        # Loop over each time step in the target sequence\n",
    "        # (excluding the start token we gave, and excluding the final EOS because we will predict that)\n",
    "        target_length = targets.size(1)\n",
    "        loss = 0.0\n",
    "        for t in range(0, target_length):\n",
    "            # One decoder step\n",
    "            logits, decoder_hidden, attn_weights = decoder.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Calculate loss against the actual next token\n",
    "            target_t = targets[:, t]  # (batch,)\n",
    "            loss += criterion(logits, target_t)\n",
    "            # Decide next input – either use teacher forcing or model's own prediction\n",
    "            use_teacher = (torch.rand(1).item() < teacher_force_ratio)\n",
    "            next_input = target_t if use_teacher else logits.argmax(dim=1)\n",
    "            decoder_input = next_input  # for next loop iteration\n",
    "        # Backpropagation\n",
    "        loss = loss / target_length   # average loss per time-step (optional scaling)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # (Optional) decay teacher_force_ratio, or compute validation loss for early stopping.\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbca82",
   "metadata": {},
   "source": [
    "In this training pseudocode:\n",
    "- We iterate over batches of sentence pairs.\n",
    "- After encoding, we set up the decoder with `<SOS>` inputs.\n",
    "- We iterate `t` from 0 to `target_length-1`. If `target_length` includes the `<EOS>` token at the end, the decoder will make a prediction for each actual token and ideally one for `<EOS>` as well. We can adjust the loop to go until `target_length-2` and then handle the last `<EOS>` prediction, but the above treats the last token in `targets` (which should be `<EOS>`) as the target to predict in the final step.\n",
    "- `criterion` is` CrossEntropyLoss` with `ignore_index` set to the pad token index, so that padded positions in the target do not contribute to loss. Only actual tokens and `<EOS>` contribute.\n",
    "- We accumulate loss across time steps and backpropagate. (PyTorch’s autograd will handle the internal gradient flow through the sequence).\n",
    "- We use teacher forcing with probability `teacher_force_ratio`. Initially this is 1 (always feed the true token). We might reduce this after a few epochs (not shown, but one could multiply `teacher_force_ratio *= 0.95` each epoch or so, to a minimum threshold).\n",
    "- We clip gradients to mitigate any exploding gradient problem due to long sequences.\n",
    "- We print the average loss per epoch. We would also typically evaluate on a validation set to monitor performance and avoid overfitting (early stopping if needed).\n",
    "\n",
    "Scientific rationale: This training approach is standard for seq2seq models. Teacher forcing greatly stabilizes training, though it can cause a discrepancy between training and inference (exposure bias, since at inference the model won’t always get the correct previous token). By gradually reducing teacher forcing, we expose the model to its own errors and make it learn to recover from them. This addresses the exposure bias to some extent (as suggested by Bengio et al. in scheduled sampling).\n",
    "\n",
    "We also ensure the model learns to output the end-of-sequence (`<EOS>`) token appropriately. The training pairs include `<EOS>` at end of targets, and we include those in the loss. This teaches the decoder when to stop. Without this, the decoder might produce unnaturally long outputs or never terminate.\n",
    "\n",
    "**Generalization and expressivity:** We rely on several aspects to ensure the model generalizes beyond simply memorizing training pairs:\n",
    "\n",
    "- **Vocabulary coverage:** Because the dataset covers different ambiguity types, the model sees multiple examples of how to resolve various ambiguities. For instance, it might see many instances of pronoun \"it\" referring to different nouns in similar contexts. The LSTM’s ability to capture context allows it to infer which noun fits even for combinations it hasn't seen before, by learning context patterns. For example, it might learn that \"it ... for culture ...\" often actually means \"no professor ... for culture ...\" from one instance, and apply that mapping in another context if appropriate.\n",
    "- **Dropout regularization:** included in embeddings or between LSTM layers (if multi-layer) will help prevent over-reliance on exact token sequences.\n",
    "- **Attention mechanism:** improves generalization by not forcing the encoder to squash all information into one vector – the decoder can adaptively fetch relevant info. This means even if a sentence is longer or structured slightly differently than seen before, the decoder can still attend to the right parts when needed, rather than being confused by extra clauses. This is essential for structural ambiguity, where the model must learn to possibly re-order or re-associate phrases. With attention, re-ordering is easier because the decoder can jump to attend to a later part of the encoder output out-of-order.\n",
    "- **Evaluation on held-out data:** We would hold out some sentence pairs as a test set. Generalization means the model should handle new ambiguous sentences and still produce correct disambiguations. We would measure this by metrics like BLEU (for overlap with the reference original sentence) or accuracy on critical words (e.g., did the model pick the correct sense or referent). The aim is that the model doesn't just parrot training outputs, but truly learns disambiguation.\n",
    "\n",
    "**Interpretability considerations:** Although neural networks are often black-box, our model has some interpretable components:\n",
    "- The **attention weights** can be visualized to understand which parts of the input the model focused on when generating each output word. This can provide insights, for example, confirming that when the model output \"lutjanus_blackfordi\", it was attending highly to the position of \"it\" in the input and perhaps surrounding descriptive words.\n",
    "- We kept the architecture relatively simple (one-layer LSTM encoder/decoder) and avoided opaque external modules. This means each part (embedding, LSTM, attention) has a clear role that can be analyzed. For instance, one could inspect the learned embeddings to see if ambiguous words and their disambiguated counterparts occupy similar regions in vector space, which would indicate the model is clustering synonyms or related concepts together.\n",
    "- Compared to more complex models (e.g., Transformers or models that disentangle syntax/semantics with separate modules), our approach is easier to trace end-to-end. This aligns with the constraint of interpretability: every step (from tokenization to output) is under our control and understandable.\n",
    "- That said, one could integrate explicit knowledge into the model for even more interpretability. For example, the recent RULER approach combines rule-based transformations with neural networks. It learns explicit rewrite rules (which are human-readable) and uses the neural model to refine the output. RULER demonstrated improved interpretability and generalization by ensuring the model learns global transformation rules not just local edits. In our design, we did not implement a rule extraction component, but we take inspiration from such work by encouraging global changes (our seq2seq is free to reorder entire clauses, not just tweak words). If we found the neural model was making only minimal changes (local modifications), we could consider incorporating a loss term or data augmentation to promote more varied rephrasings, akin to learning rules as in RULER.\n",
    "- Another line of research, like **AMR-based Paraphrase Generation (AMRPG)**, explicitly uses semantic parses (Abstract Meaning Representation) to guide paraphrasing. AMRPG separates the generation of meaning and syntax: it would parse the ambiguous sentence into an AMR graph (disambiguating meaning explicitly) and then realize it into a sentence, possibly with a target syntax. This approach can inherently solve ambiguities because the AMR graph is a clarified representation (e.g., pronouns resolved, roles clarified). However, it requires an AMR parser and generator, which are external tools and introduce complexity. We chose not to go this route due to the requirement of minimal external dependencies, but we acknowledge that such techniques could improve performance and interpretability (since the intermediate AMR is inspectable). Instead, our model must learn an implicit internal \"representation\" of meaning with its encoder. The attention mechanism and hidden states will approximate what an explicit semantic representation might have provided.\n",
    "\n",
    "After sufficient training (monitoring validation loss for convergence), the model should be able to reconstruct disambiguated sentences from ambiguous inputs reliably.\n",
    "\n",
    "\n",
    "## Inference and Example\n",
    "\n",
    "At inference (test) time, we feed an ambiguous sentence into the encoder, then use the decoder to generate the output until `<EOS>` is produced. We do not use teacher forcing in inference; instead, at each step the decoder’s own prediction is fed as the next input. We can use greedy decoding (always pick highest probability token) or beam search for potentially better results. Here we illustrate greedy decoding for simplicity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepended specials: ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
      "Total unique tokens from CSV: 483\n",
      "New vocab size = 487\n",
      "Vocab size = 487, #examples = 74997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import ast\n",
    "\n",
    "# --- 1) Load raw vocab.csv (columns: 'primary', 'secondary') and flatten all tokens ---\n",
    "vocab_df = pd.read_csv('data/vocab.csv')\n",
    "raw_tokens = set()\n",
    "\n",
    "for col in ['primary', 'secondary']:\n",
    "    for cell in vocab_df[col].dropna():\n",
    "        # each cell is a string like \"['and','but',...']\"\n",
    "        try:\n",
    "            lst = ast.literal_eval(cell)\n",
    "            if isinstance(lst, list):\n",
    "                raw_tokens.update(lst)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# --- 1a) Define & prepend special tokens ---\n",
    "specials = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "# Ensure no overlap:\n",
    "for s in specials:\n",
    "    raw_tokens.discard(s)\n",
    "\n",
    "# Final ordered vocab: specials first, then sorted rest\n",
    "all_tokens = specials + sorted(raw_tokens)\n",
    "\n",
    "# --- 1b) Rebuild mappings from scratch ---\n",
    "word2index = {tok: idx for idx, tok in enumerate(all_tokens)}\n",
    "index2word = {idx: tok for tok, idx in word2index.items()}\n",
    "vocab_size = len(all_tokens)\n",
    "\n",
    "print(f\"Prepended specials: {specials}\")\n",
    "print(f\"Total unique tokens from CSV: {len(raw_tokens)}\")\n",
    "print(f\"New vocab size = {vocab_size}\")\n",
    "\n",
    "# --- 2) Simple tokenizer: keeps alphanumeric + underscore tokens together,\n",
    "#    splits off punctuation.\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "# --- 3) Dataset that reads final_dataset.csv and returns (input_ids, target_ids)\n",
    "class FinalDataset(Dataset):\n",
    "    def __init__(self, csv_path, w2i, tokenizer):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.amb = df['ambiguous_sentence'].astype(str).tolist()\n",
    "        self.orig = df['original_sentence'].astype(str).tolist()\n",
    "        self.w2i = w2i\n",
    "        self.tok = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.amb)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inp = self.tok(self.amb[i])\n",
    "        tgt = self.tok(self.orig[i])\n",
    "        inp_ids = [self.w2i.get(w, self.w2i['<UNK>']) for w in inp]\n",
    "        # prepend SOS, append EOS for target\n",
    "        tgt_ids = [self.w2i['<SOS>']] + \\\n",
    "                  [self.w2i.get(w, self.w2i['<UNK>']) for w in tgt] + \\\n",
    "                  [self.w2i['<EOS>']]\n",
    "        return torch.tensor(inp_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "# --- 4) Collate fn: pad to longest in batch ---\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inp_pad = pad_sequence(inputs, batch_first=True, padding_value=word2index['<PAD>'])\n",
    "    tgt_pad = pad_sequence(targets, batch_first=True, padding_value=word2index['<PAD>'])\n",
    "    return inp_pad, tgt_pad\n",
    "\n",
    "# --- 5) Build DataLoader ---\n",
    "dataset = FinalDataset('data/final_dataset.csv', word2index, tokenize)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Vocab size = {vocab_size}, #examples = {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d37fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Device setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- 2) Model definitions (reuse classes you already have) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2index['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)                    # (B, L_in, E)\n",
    "        outputs, hidden = self.lstm(emb)           # outputs=(B,L_in,H), hidden=(h_n,c_n)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2index['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.out  = nn.Linear(hidden_size*2, vocab_size)\n",
    "    def forward_step(self, prev_tok, hidden, enc_outputs):\n",
    "        emb = self.embedding(prev_tok).unsqueeze(1)     # (B,1,E)\n",
    "        out, hidden = self.lstm(emb, hidden)            # out=(B,1,H)\n",
    "        # dot-product attention\n",
    "        scores = torch.bmm(out, enc_outputs.transpose(1,2))  # (B,1,L_in)\n",
    "        attn  = torch.softmax(scores, dim=2)                # (B,1,L_in)\n",
    "        ctx   = torch.bmm(attn, enc_outputs).squeeze(1)     # (B,H)\n",
    "        out_t = out.squeeze(1)                              # (B,H)\n",
    "        cat   = torch.cat([out_t, ctx], dim=1)              # (B,2H)\n",
    "        logits= self.out(cat)                               # (B,V)\n",
    "        return logits, hidden, attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08eb10",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# --- 1) Load raw vocab.csv (columns: 'primary', 'secondary') and flatten all tokens ---\n",
    "vocab_df = pd.read_csv('vocab.csv')\n",
    "raw_tokens = set()\n",
    "\n",
    "for col in ['primary', 'secondary']:\n",
    "    for cell in vocab_df[col].dropna():\n",
    "        # each cell is a string like \"['and','but',...']\"\n",
    "        try:\n",
    "            lst = ast.literal_eval(cell)\n",
    "            if isinstance(lst, list):\n",
    "                raw_tokens.update(lst)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# --- 1a) Define & prepend special tokens ---\n",
    "specials = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "# Ensure no overlap:\n",
    "for s in specials:\n",
    "    raw_tokens.discard(s)\n",
    "\n",
    "# Final ordered vocab: specials first, then sorted rest\n",
    "all_tokens = specials + sorted(raw_tokens)\n",
    "\n",
    "# --- 1b) Rebuild mappings from scratch ---\n",
    "word2index = {tok: idx for idx, tok in enumerate(all_tokens)}\n",
    "index2word = {idx: tok for tok, idx in word2index.items()}\n",
    "vocab_size = len(all_tokens)\n",
    "\n",
    "print(f\"Prepended specials: {specials}\")\n",
    "print(f\"Total unique tokens from CSV: {len(raw_tokens)}\")\n",
    "print(f\"New vocab size = {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "Train/Val split: 67498/7499 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b708dd89064a96af168297142f75da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m         dec_input \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m tgt_len\n\u001b[1;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(params, max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n\u001b[0;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\claza\\anaconda3\\envs\\NLP2025\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.notebook import tqdm  # for progress bars\n",
    "\n",
    "# ---- 1) Reproducibility ----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---- 2) Device ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device:\", device)\n",
    "\n",
    "# ---- 3) Dataset & Split ----\n",
    "# Assume 'dataset', 'collate_fn', and 'FinalDataset' are defined in earlier cells\n",
    "total_size = len(dataset)\n",
    "val_size = int(0.1 * total_size)\n",
    "train_size = total_size - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "\n",
    "print(f\"Train/Val split: {train_size}/{val_size} examples\")\n",
    "\n",
    "# ---- 4) Model Instantiation ----\n",
    "# Reuse Encoder and Decoder from earlier cells\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size).to(device)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "# ---- 5) Optimizer & Loss ----\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2index['<PAD>'])\n",
    "\n",
    "# ---- 6) Training Loop ----\n",
    "num_epochs = 10\n",
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    encoder.train(); decoder.train()\n",
    "    train_loss = 0.0\n",
    "    for inp, tgt in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        enc_out, enc_hidden = encoder(inp)\n",
    "        dec_hidden = enc_hidden\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        dec_input = torch.full((batch_size,), word2index['<SOS>'], dtype=torch.long, device=device)\n",
    "        \n",
    "        loss = 0.0\n",
    "        for t in range(tgt_len):\n",
    "            logits, dec_hidden, _ = decoder.forward_step(dec_input, dec_hidden, enc_out)\n",
    "            true_tok = tgt[:, t]\n",
    "            loss += criterion(logits, true_tok)\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                dec_input = true_tok\n",
    "            else:\n",
    "                dec_input = logits.argmax(dim=1)\n",
    "        loss = loss / tgt_len\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loss\n",
    "    encoder.eval(); decoder.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in val_loader:\n",
    "            inp, tgt = inp.to(device), tgt.to(device)\n",
    "            enc_out, enc_hidden = encoder(inp)\n",
    "            dec_hidden = enc_hidden\n",
    "            batch_size, tgt_len = tgt.size()\n",
    "            dec_input = torch.full((batch_size,), word2index['<SOS>'], dtype=torch.long, device=device)\n",
    "            loss = 0.0\n",
    "            for t in range(tgt_len):\n",
    "                logits, dec_hidden, _ = decoder.forward_step(dec_input, dec_hidden, enc_out)\n",
    "                true_tok = tgt[:, t]\n",
    "                loss += criterion(logits, true_tok)\n",
    "                dec_input = true_tok  # always teacher-forcing on val\n",
    "            val_loss += (loss / tgt_len).item()\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch} — Train Loss: {avg_train:.4f}, Val Loss: {avg_val:.4f}\")\n",
    "    # Optionally decay teacher forcing: teacher_forcing_ratio *= 0.95\n",
    "\n",
    "# ---- 7) Save Models ----\n",
    "torch.save(encoder.state_dict(), 'models/encoder_final.pt')\n",
    "torch.save(decoder.state_dict(), 'models/decoder_final.pt')\n",
    "print(\"Models saved: encoder_final.pt, decoder_final.pt\")\n",
    "\n",
    "# ---- 8) Inference & Samples ----\n",
    "special_tokens = {'<PAD>','<SOS>','<EOS>','<UNK>'}\n",
    "\n",
    "def generate(sentence, max_len=50):\n",
    "    \"\"\"\n",
    "    1) Masks out specials so the model never picks them.\n",
    "    2) If it still outputs <UNK>, we use the attention weights\n",
    "       to pick the source token it was “looking at” most.\n",
    "    3) We filter out any <SOS> or <PAD> in the final list.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    # tokenize & indices\n",
    "    src_tokens = tokenize(sentence)\n",
    "    src_ids    = [word2index.get(w, word2index['<UNK>']) for w in src_tokens]\n",
    "    inp        = torch.tensor(src_ids, device=device).unsqueeze(0)\n",
    "    # encode\n",
    "    enc_out, enc_hidden = encoder(inp)\n",
    "    dec_hidden = enc_hidden\n",
    "    # start decoding\n",
    "    dec_input  = torch.tensor([word2index['<SOS>']], device=device)\n",
    "    out_tokens = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # get logits and attention\n",
    "        logits, dec_hidden, attn = decoder.forward_step(dec_input, dec_hidden, enc_out)\n",
    "        # mask out specials so they have zero probability\n",
    "        for sp in ['<PAD>','<SOS>','<UNK>']:\n",
    "            logits[:, word2index[sp]] = -1e9\n",
    "        # pick next\n",
    "        next_id = logits.argmax(dim=1).item()\n",
    "        if next_id == word2index['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        tok = index2word[next_id]\n",
    "        # if it’s still <UNK>, copy from source via attention\n",
    "        if tok == '<UNK>':\n",
    "            # attn shape is (1,1,src_len)\n",
    "            a = attn.squeeze(0).squeeze(0)          # (src_len,)\n",
    "            src_pos = a.argmax().item()\n",
    "            tok = src_tokens[src_pos]\n",
    "        \n",
    "        # only keep real tokens\n",
    "        if tok not in special_tokens:\n",
    "            out_tokens.append(tok)\n",
    "        \n",
    "        dec_input = torch.tensor([next_id], device=device)\n",
    "    \n",
    "    return \" \".join(out_tokens)\n",
    "\n",
    "print(\"\\nSample generations on validation set:\")\n",
    "for i in range(5):\n",
    "    amb, orig = val_ds[i]\n",
    "    print(f\"\\nAmbiguous: {' '.join(index2word.get(x.item(),'<UNK>') for x in amb)}\")\n",
    "    print(\"Generated:\", generate(' '.join(index2word.get(x.item(),'<UNK>') for x in amb)))\n",
    "    print(\"Reference:\", ' '.join(index2word.get(x.item(),'<UNK>') for x in orig[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load decoder_final and encoder_final\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size)\n",
    "encoder.load_state_dict(torch.load('models/encoder_final.pt', map_location=device))\n",
    "decoder.load_state_dict(torch.load('models/decoder_final.pt', map_location=device))\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "special_tokens = {'<PAD>','<SOS>','<EOS>','<UNK>'}\n",
    "def generate(sentence, max_len=50):\n",
    "    \"\"\"\n",
    "    1) Masks out specials so the model never picks them.\n",
    "    2) If it still outputs <UNK>, we use the attention weights\n",
    "       to pick the source token it was “looking at” most.\n",
    "    3) We filter out any <SOS> or <PAD> in the final list.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    # tokenize & indices\n",
    "    src_tokens = tokenize(sentence)\n",
    "    src_ids    = [word2index.get(w, word2index['<UNK>']) for w in src_tokens]\n",
    "    inp        = torch.tensor(src_ids, device=device).unsqueeze(0)\n",
    "    # encode\n",
    "    enc_out, enc_hidden = encoder(inp)\n",
    "    dec_hidden = enc_hidden\n",
    "    # start decoding\n",
    "    dec_input  = torch.tensor([word2index['<SOS>']], device=device)\n",
    "    out_tokens = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # get logits and attention\n",
    "        logits, dec_hidden, attn = decoder.forward_step(dec_input, dec_hidden, enc_out)\n",
    "        # mask out specials so they have zero probability\n",
    "        for sp in ['<PAD>','<SOS>','<UNK>']:\n",
    "            logits[:, word2index[sp]] = -1e9\n",
    "        # pick next\n",
    "        next_id = logits.argmax(dim=1).item()\n",
    "        if next_id == word2index['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        tok = index2word[next_id]\n",
    "        # if it’s still <UNK>, copy from source via attention\n",
    "        if tok == '<UNK>':\n",
    "            # attn shape is (1,1,src_len)\n",
    "            a = attn.squeeze(0).squeeze(0)          # (src_len,)\n",
    "            src_pos = a.argmax().item()\n",
    "            tok = src_tokens[src_pos]\n",
    "        \n",
    "        # only keep real tokens\n",
    "        if tok not in special_tokens:\n",
    "            out_tokens.append(tok)\n",
    "        \n",
    "        dec_input = torch.tensor([next_id], device=device)\n",
    "    \n",
    "    return \" \".join(out_tokens)\n",
    "\n",
    "sent1 = \"Hope you too, to enjoy it as my deepest wishes.\"\n",
    "sent2 = \"Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again.\"\n",
    "sentences = [sent1, sent2]\n",
    "for sent in sentences:\n",
    "    print(f\"Input: {sent}\")\n",
    "    print(\"Generated:\", generate(sent))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
