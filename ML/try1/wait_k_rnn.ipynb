{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63268b12",
   "metadata": {},
   "source": [
    "# RNN-based Seq2Seq Model for Sentence Disambiguation\n",
    "## Introduction and Problem Overview\n",
    "Sentence disambiguation can be framed as a **paraphrase generation** task: we take an ambiguous sentence and generate a rephrased version that resolves ambiguities (lexical, structural, referential) while preserving the original meaning.\n",
    "\n",
    "We will implement a **recurrent neural network (RNN)** based encoder-decoder (seq2seq) model in PyTorch, largely from scratch (no high-level seq2seq libraries). Our design emphasizes:\n",
    "- **Minimal external dependencies:** We'll use only PyTorch and Python standard libraries, writing our own tokenizer, data pipeline, and network modules.\n",
    "- **Flexibility in rephrasing:** The model is not constrained to copy input tokens exactly; it can learn to produce different words or reorder phrases to resolve ambiguity.\n",
    "- **Expressivity for disambiguation:** We use an architecture (with choices like LSTM units and an attention mechanism) capable of capturing context and meaning needed to handle lexical choice, structural reordering, and pronoun resolution.\n",
    "- **Scientific rigor:** Each design choice (embedding size, hidden layer type/size, attention, etc.) is justified with reference to established research or best practices.\n",
    "- **Device adaptability:** The implementation will automatically use GPU if available, falling back to CPU gracefully.\n",
    "- **Consistency with provided preprocessing:** We will follow the same tokenization and vocabulary construction approach as in the provided `preprocessing.ipynb/vocab_lookup`, ensuring our data pipeline (e.g. handling of special tokens, casing, and underscores) matches the intended setup.\n",
    "\n",
    "\n",
    "#### for detailed information on the architecture refer to `rnn.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b5256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepended specials: ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
      "Total unique tokens from CSV: 483\n",
      "New vocab size = 487\n",
      "Train examples: 1103032, Val examples: 122559 on device cpu\n",
      "Vocab size = 487, #examples = 1225591\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "\n",
    "# --- 1) Load raw vocab.csv (columns: 'primary', 'secondary') and flatten all tokens ---\n",
    "vocab_df = pd.read_csv('data/vocab.csv')\n",
    "raw_tokens = set()\n",
    "\n",
    "for col in ['primary', 'secondary']:\n",
    "    for cell in vocab_df[col].dropna():\n",
    "        # each cell is a string like \"['and','but',...']\"\n",
    "        try:\n",
    "            lst = ast.literal_eval(cell)\n",
    "            if isinstance(lst, list):\n",
    "                raw_tokens.update(lst)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# --- 1a) Define & prepend special tokens ---\n",
    "specials = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "# Ensure no overlap:\n",
    "for s in specials:\n",
    "    raw_tokens.discard(s)\n",
    "\n",
    "# Final ordered vocab: specials first, then sorted rest\n",
    "all_tokens = specials + sorted(raw_tokens)\n",
    "\n",
    "# --- 1b) Rebuild mappings from scratch ---\n",
    "word2index = {tok: idx for idx, tok in enumerate(all_tokens)}\n",
    "index2word = {idx: tok for tok, idx in word2index.items()}\n",
    "vocab_size = len(all_tokens)\n",
    "\n",
    "print(f\"Prepended specials: {specials}\")\n",
    "print(f\"Total unique tokens from CSV: {len(raw_tokens)}\")\n",
    "print(f\"New vocab size = {vocab_size}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 6) Define Dataset class ---\n",
    "class FinalDataset(Dataset):\n",
    "    def __init__(self, csv_path, w2i, tokenizer):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.src_prefixes = df['source_prefix'].astype(str).tolist()\n",
    "        self.prev_tgts    = df['prev_target'].astype(str).tolist()\n",
    "        self.next_tgts    = df['target_word'].astype(str).tolist()\n",
    "        self.w2i = w2i\n",
    "        self.tokenize = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_prefixes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tokenize(self.src_prefixes[idx])\n",
    "        prev = self.prev_tgts[idx]\n",
    "        nxt  = self.next_tgts[idx]\n",
    "        src_ids  = [ self.w2i.get(t, self.w2i['<UNK>']) for t in src ]\n",
    "        prev_id  = self.w2i.get(prev, self.w2i['<UNK>'])\n",
    "        next_id  = self.w2i.get(nxt,  self.w2i['<UNK>'])\n",
    "        return torch.tensor(src_ids, dtype=torch.long), prev_id, next_id\n",
    "# --- 7) Define collate_fn for padding sequences ---\n",
    "def collate_fn(batch):\n",
    "    src_seqs, prev_ids, next_ids = zip(*batch)\n",
    "    src_lens = [len(s) for s in src_seqs]\n",
    "    src_pad  = pad_sequence(src_seqs, batch_first=True, padding_value=word2index['<PAD>'])\n",
    "    return (\n",
    "        src_pad,\n",
    "        torch.tensor(src_lens, dtype=torch.long),\n",
    "        torch.tensor(prev_ids, dtype=torch.long),\n",
    "        torch.tensor(next_ids, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "batch_size = 128\n",
    "K = 3  \n",
    "\n",
    "# 8) Load & split\n",
    "dataset2 = FinalDataset('data/final_dataset_2.csv', word2index, tokenize)\n",
    "val_size = int(0.1 * len(dataset2))\n",
    "train_size = len(dataset2) - val_size\n",
    "train_ds, val_ds = random_split(dataset2, [train_size, val_size],\n",
    "                                 generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader2 = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader2   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train examples: {len(train_ds)}, Val examples: {len(val_ds)} on device {device}\")\n",
    "\n",
    "# --- 8) Build DataLoader ---\n",
    "dataset = FinalDataset('data/final_dataset_2.csv', word2index, tokenize)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Vocab size = {vocab_size}, #examples = {len(dataset)}\")\n",
    "\n",
    "# --- 9) Model definition ---\n",
    "# --- 1) Device setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- 1) Device setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- 2) Model definitions (reuse classes you already have) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2index['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)                    # (B, L_in, E)\n",
    "        outputs, hidden = self.lstm(emb)           # outputs=(B,L_in,H), hidden=(h_n,c_n)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2index['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.out  = nn.Linear(hidden_size*2, vocab_size)\n",
    "    def forward_step(self, prev_tok, hidden, enc_outputs):\n",
    "        emb = self.embedding(prev_tok).unsqueeze(1)     # (B,1,E)\n",
    "        out, hidden = self.lstm(emb, hidden)            # out=(B,1,H)\n",
    "        # dot-product attention\n",
    "        scores = torch.bmm(out, enc_outputs.transpose(1,2))  # (B,1,L_in)\n",
    "        attn  = torch.softmax(scores, dim=2)                # (B,1,L_in)\n",
    "        ctx   = torch.bmm(attn, enc_outputs).squeeze(1)     # (B,H)\n",
    "        out_t = out.squeeze(1)                              # (B,H)\n",
    "        cat   = torch.cat([out_t, ctx], dim=1)              # (B,2H)\n",
    "        logits= self.out(cat)                               # (B,V)\n",
    "        return logits, hidden, attn\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3260316",
   "metadata": {},
   "source": [
    "We train the same encoder–decoder with attention on these prefix examples.  For each example \\(i\\) with source prefix \\(x_{1:r_i}\\) and previous target \\(y_{t-1}^{(i)}\\):\n",
    "\n",
    "1. **Encode prefix**  \n",
    "   $$\n",
    "     (h_{1:r_i}, c_{1:r_i})\n",
    "     = \\mathrm{Encoder}(x_{1:r_i}).\n",
    "   $$\n",
    "\n",
    "2. **Single-step decode**  \n",
    "   $$\n",
    "     \\tilde{h}_t\n",
    "     = \\mathrm{DecoderStep}\\bigl(y_{t-1},\\,h_{r_i},\\,c_{r_i},\\,h_{1:r_i}\\bigr).\n",
    "   $$\n",
    "\n",
    "3. **Dot-product attention**  \n",
    "   $$\n",
    "     \\alpha_j\n",
    "     = \\frac{\\exp(\\tilde{h}_t^\\top h_j)}\n",
    "            {\\sum_{k=1}^{r_i}\\exp(\\tilde{h}_t^\\top h_k)},\\quad\n",
    "     c_t = \\sum_{j=1}^{r_i}\\alpha_j\\,h_j.\n",
    "   $$\n",
    "\n",
    "4. **Prediction & loss**  \n",
    "   $$\n",
    "     \\hat y_t\n",
    "     = \\arg\\max\\mathrm{Softmax}\\bigl(W[\\tilde{h}_t; c_t]+b\\bigr),\\quad\n",
    "     \\mathcal{L}\n",
    "     = -\\sum_i \\log p_\\theta\\bigl(y_t^{(i)}\\mid x_{1:r_i}^{(i)},\\,y_{<t}^{(i)}\\bigr).\n",
    "   $$\n",
    "\n",
    "We backpropagate this cross-entropy loss and update all model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a35153a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f9b513f2884bc587400782342b81a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 → Train Loss: 1.1899, Val Loss: 0.6422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8c3118edaa400e9ac88558eb98ce65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 → Train Loss: 0.5886, Val Loss: 0.5733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c3fe7121b7488187155fb5300fb6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 → Train Loss: 0.5332, Val Loss: 0.5414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcd61c055ad4261a3b78c3819c29ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 → Train Loss: 0.4970, Val Loss: 0.5251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7072651b94ff444782820a44f5550c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 → Train Loss: 0.4711, Val Loss: 0.5085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba32e23b16c84b059a83a4082d9b53df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 → Train Loss: 0.4497, Val Loss: 0.4960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf710a50be943bc82dad42b561e5c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 → Train Loss: 0.4326, Val Loss: 0.4871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4322f29e7a4d869ff071e4ded12950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 → Train Loss: 0.4180, Val Loss: 0.4771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76869625d824d8c9bcee874724d62c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 → Train Loss: 0.4052, Val Loss: 0.4733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72598abe51414338b54a4ddd9a4e699a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Training:   0%|          | 0/8618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 → Train Loss: 0.3941, Val Loss: 0.4670\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 1) Hyperparameters    \n",
    "embed_size = 400\n",
    "hidden_size = 128\n",
    "num_epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "# 2) Model instantiation (reuse your Encoder/Decoder classes)\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size).to(device)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "# 3) Optimizer & loss\n",
    "params    = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2index['<PAD>'])\n",
    "\n",
    "# 4) Training & Validation\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    encoder.train(); decoder.train()\n",
    "    total_train = 0.0\n",
    "    for src_pad, src_lens, prev_ids, tgt_ids in tqdm(train_loader2, desc=f\"Epoch {epoch} Training\"):\n",
    "        src_pad, src_lens = src_pad.to(device), src_lens.to(device)\n",
    "        prev_ids, tgt_ids = prev_ids.to(device), tgt_ids.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # encode prefix\n",
    "        enc_out, enc_hidden = encoder(src_pad)\n",
    "        # one-step decode\n",
    "        logits, dec_hidden, _ = decoder.forward_step(prev_ids, enc_hidden, enc_out)\n",
    "        loss = criterion(logits, tgt_ids)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_train += loss.item()\n",
    "    avg_train = total_train / len(train_loader2)\n",
    "\n",
    "    encoder.eval(); decoder.eval()\n",
    "    total_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src_pad, src_lens, prev_ids, tgt_ids in val_loader2:\n",
    "            src_pad, src_lens = src_pad.to(device), src_lens.to(device)\n",
    "            prev_ids, tgt_ids = prev_ids.to(device), tgt_ids.to(device)\n",
    "            enc_out, enc_hidden = encoder(src_pad)\n",
    "            logits, dec_hidden, _ = decoder.forward_step(prev_ids, enc_hidden, enc_out)\n",
    "            total_val += criterion(logits, tgt_ids).item()\n",
    "    avg_val = total_val / len(val_loader2)\n",
    "\n",
    "    print(f\"Epoch {epoch} → Train Loss: {avg_train:.4f}, Val Loss: {avg_val:.4f}\")\n",
    "# --- 10) Save each model seperately---\n",
    "torch.save(encoder.state_dict(), 'models/encoder_2.pth')\n",
    "torch.save(decoder.state_dict(), 'models/decoder_2.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf05feb",
   "metadata": {},
   "source": [
    "At test time we generate one token at a time under the same Wait-$K$ schedule:\n",
    "\n",
    "- Initialize $t=1$, $r(1)=\\min(K, L_x)$, and $\\hat y_0=\\texttt{<sos>}$.  \n",
    "- Repeat until $\\hat y_{t-1}=\\texttt{<eos>}$ or $t>\\text{max\\_len}$:\n",
    "\n",
    "  1. **Encode prefix**  \n",
    "     $$\n",
    "       (h_{1:r(t)}, c_{1:r(t)})\n",
    "       = \\mathrm{Encoder}(x_{1:r(t)}).\n",
    "     $$\n",
    "\n",
    "  2. **Decode one step**  \n",
    "     $$\n",
    "       \\tilde{h}_t\n",
    "       = \\mathrm{DecoderStep}\\bigl(\\hat y_{t-1},\\,h_{r(t)},\\,c_{r(t)},\\,h_{1:r(t)}\\bigr).\n",
    "     $$\n",
    "\n",
    "  3. **Attention & predict**  \n",
    "     $$\n",
    "       \\alpha_j = \\frac{\\exp(\\tilde{h}_t^\\top h_j)}\n",
    "                       {\\sum_{k=1}^{r(t)}\\exp(\\tilde{h}_t^\\top h_k)},\\quad\n",
    "       c_t = \\sum_{j=1}^{r(t)}\\alpha_j\\,h_j,\n",
    "     $$\n",
    "     $$\n",
    "       \\hat y_t = \\arg\\max\\mathrm{Softmax}\\bigl(W[\\tilde{h}_t; c_t]+b\\bigr).\n",
    "     $$\n",
    "\n",
    "  4. **Advance**  \n",
    "     $t \\leftarrow t+1$ and  \n",
    "     $r(t)=\\min(K+(t-1),L_x)$.\n",
    "\n",
    "The output sequence $\\hat y_{1:T}$ is generated under the desired latency constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9271daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our dragon boat festival , in our chinese culture , to celebrate trapezoidal message with all safe and great in our st._john_chrysostom with all st._john_chrysostom with all st._john_chrysostom with all\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_size = 400\n",
    "hidden_size = 128\n",
    "lr = 1e-3\n",
    "special_tokens = {'<PAD>','<SOS>','<EOS>','<UNK>'}\n",
    "# load the encoder and decoder models from the models directory\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size).to(device)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size).to(device)\n",
    "encoder.load_state_dict(torch.load('models/encoder_2.pth'))\n",
    "decoder.load_state_dict(torch.load('models/decoder_2.pth'))\n",
    "\n",
    "def translate_wait_k(src_sentence, K, max_len=50):\n",
    "    encoder.eval(); decoder.eval()\n",
    "    src_toks = tokenize(src_sentence)\n",
    "    src_ids  = [ word2index.get(t, word2index['<UNK>']) for t in src_toks ]\n",
    "    outputs = []\n",
    "    t = 1\n",
    "    while True:\n",
    "        r = min(K + (t-1), len(src_ids))\n",
    "        inp = torch.tensor(src_ids[:r], device=device).unsqueeze(0)\n",
    "        enc_out, enc_hidden = encoder(inp)\n",
    "        prev_id = word2index['<SOS>'] if t == 1 else outputs[-1]\n",
    "        logits, dec_hidden, attn = decoder.forward_step(\n",
    "            torch.tensor([prev_id], device=device), enc_hidden, enc_out\n",
    "        )\n",
    "        # mask specials\n",
    "        for sp in special_tokens - {'<EOS>'}:\n",
    "            logits[:, word2index[sp]] = -1e9\n",
    "        next_id = logits.argmax(dim=1).item()\n",
    "        if next_id == word2index['<EOS>'] or t >= max_len:\n",
    "            break\n",
    "        tok = index2word[next_id]\n",
    "        # copy from source if UNK\n",
    "        if tok == '<UNK>':\n",
    "            a = attn.squeeze(0).squeeze(0)\n",
    "            src_pos = a.argmax().item()\n",
    "            tok = src_toks[src_pos]\n",
    "            next_id = word2index.get(tok, next_id)\n",
    "        if tok not in special_tokens:\n",
    "            outputs.append(next_id)\n",
    "        else:\n",
    "            outputs.append(next_id)\n",
    "        t += 1\n",
    "\n",
    "    return \" \".join(index2word[i] for i in outputs)\n",
    "\n",
    "# Sample usage:\n",
    "sentence = \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives.\"\n",
    "print(translate_wait_k(sentence, K=3, max_len=len(sentence.split())+10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
