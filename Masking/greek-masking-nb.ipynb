{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Clause Input Task – Structured Legal Text Completion in Greek\n",
    "### Introduction\n",
    "In this notebook, we tackle a specialized Masked Language Modeling (MLM) task within the legal domain, specifically focusing on Masked Clause Input. This involves completing missing words from legal clauses written in Greek, where the ultimate goal is to **evaluate open-source language** models based on their **semantic fidelity** to the Greek Civil Code (Αστικός Κώδικας).\n",
    "\n",
    "---\n",
    "\n",
    "### Task Description\n",
    "We are provided with incomplete legal clauses where one or more words are masked. Our goal is to:\n",
    "- Predict the missing words in Greek\n",
    "- Use open-source language models \n",
    "- Evaluate and compare their performance\n",
    "This task is classified under **Masked Clause Input**, a variant of classic masked language modeling with emphasis on clause-level legal text.\n",
    "\n",
    "---\n",
    "\n",
    "### Models and Papers Used\n",
    "\n",
    "- [BERT Multilingual Cased](https://huggingface.co/google-bert/bert-base-multilingual-cased)  \n",
    "  [Paper: BERT – Devlin et al. (2018)](https://arxiv.org/pdf/1810.04805)\n",
    "\n",
    "- [DistilBERT Multilingual Cased](https://huggingface.co/distilbert/distilbert-base-multilingual-cased)  \n",
    "  [Paper: DistilBERT – Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108)\n",
    "\n",
    "- [XLM-RoBERTa Base](https://huggingface.co/FacebookAI/xlm-roberta-base)  \n",
    "  [Paper: XLM-R – Conneau et al. (2019)](https://arxiv.org/pdf/1911.02116)\n",
    "\n",
    "- [InfoXLM Base](https://huggingface.co/microsoft/infoxlm-base)  \n",
    "  [Paper: InfoXLM – Chi et al. (2020)](https://arxiv.org/pdf/2007.07834)\n",
    "\n",
    "- [GreekBERT (NLPAUEB)](https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1)  \n",
    "  [Paper: Koutsikakis et al. (2020)](https://arxiv.org/pdf/2008.12014)\n",
    "\n",
    "- [GreekSocialBERT](https://huggingface.co/gealexandri/greeksocialbert-base-greek-uncased-v1)  \n",
    "  [Paper: Alexandri et al. (2021)](https://www.mdpi.com/2078-2489/12/8/331)\n",
    "\n",
    "- [GreekLegalRoBERTa v3](https://huggingface.co/AI-team-UoA/GreekLegalRoBERTa_v3)  \n",
    "  [Paper: Chronopoulou et al. (2024)](https://arxiv.org/pdf/2410.12852)\n",
    "\n",
    "- [Llama-Krikri 8B](https://huggingface.co/ilsp/Llama-Krikri-8B-Base)  \n",
    "  [Paper: ILSP Krikri LLaMA (2024)](https://arxiv.org/pdf/2502.01534)\n",
    "\n",
    "- [mT5 Base](https://huggingface.co/google/mt5-base)  \n",
    "  [Paper: Xue et al. (2020)](https://arxiv.org/pdf/2010.11934)\n",
    "\n",
    "- [Meltemi-7B v1.5](https://huggingface.co/ilsp/Meltemi-7B-v1.5)  \n",
    "  [Paper: ILSP Meltemi (2024)](https://arxiv.org/pdf/2407.20743)\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics for Masked Token Prediction in Greek Legal Clauses\n",
    "\n",
    "In evaluating masked token predictions on Greek Civil Code clauses, we focus on two aspects:\n",
    "1. **Exact-match accuracy** (did the model restore the exact original legal term?), and  \n",
    "2. **Semantic alignment** (does the clause retain the same meaning even if a different token is used?).  \n",
    "\n",
    "Below we adapt standard NLP metrics to this task, emphasizing precision in legal terminology and overall clause meaning. All metrics are computed only over the **masked tokens**, since the unmasked context remains unchanged. In legal texts, small wording differences can alter interpretation, so our definitions consider the strict requirements of legal language while also allowing measures of semantic similarity for acceptable paraphrases.\n",
    "\n",
    "---\n",
    "\n",
    "### Token-Level Precision, Recall & F₁ (Masked-Token Exact Match)\n",
    "\n",
    "We treat each masked token prediction as an attempt to recover the ground-truth legal word. Define:  \n",
    "- $TP$ = number of **correctly predicted** tokens (exact match with the ground truth).  \n",
    "- $FP$ = number of **incorrect predictions** (predicted token ≠ ground truth).  \n",
    "- $FN$ = number of ground-truth tokens the model failed to predict correctly.  \n",
    "\n",
    "Then:\n",
    "\n",
    "- **Precision**  \n",
    "  $$\n",
    "    P \\;=\\; \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "\n",
    "- **Recall**  \n",
    "  $$\n",
    "    R \\;=\\; \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "\n",
    "- **F₁ Score**  \n",
    "  $$\n",
    "    F_{1} \\;=\\; 2 \\cdot \\frac{P \\cdot R}{P + R}\n",
    "  $$\n",
    "\n",
    "Because each mask corresponds to exactly one ground-truth token, $FP = FN$ whenever a single token is predicted per mask. In that scenario, precision $=$ recall $=$ exact-match accuracy, and thus $F_1 = P = R$. However, if multiple valid synonyms or morphological variants are allowed (rare in legal evaluation), recall will reflect coverage of all acceptable predictions.  \n",
    "\n",
    "> **Legal Note:** In legal text, **exact matches are typically required**—a predicted token is correct only if it exactly matches the ground truth term (e.g.\\ “σύμβαση” vs. “συμφωνία” would be considered different, even if semantically related). For strict legal evaluation, we count only verbatim matches.  \n",
    ">  \n",
    "> *See Rajpurkar et al. (2016) for token-level F₁ in QA benchmarks*  \n",
    "> [Paper: SQuAD (Rajpurkar et al., 2016)](https://arxiv.org/abs/1606.05250)  \n",
    "> *See Chalkidis et al. (2020) for LegalBERT exact-match evaluation in legal NLP*  \n",
    "> [Paper: LegalBERT (Chalkidis et al., 2020)](https://arxiv.org/abs/2010.02559)  \n",
    "\n",
    "---\n",
    "\n",
    "### Word Error Rate (WER) for Masked-Token Errors\n",
    "\n",
    "**WER** measures the normalized edit distance between the **predicted clause** (with filled-in tokens) and the **ground truth clause**. Let:  \n",
    "- $S$ = number of substitutions (wrong token for the correct one),  \n",
    "- $D$ = number of deletions (ground truth token not predicted),  \n",
    "- $I$ = number of insertions (extra token added),  \n",
    "- $N$ = total number of words in the ground truth clause.  \n",
    "\n",
    "Then:\n",
    "$$\n",
    "  \\mathrm{WER} \\;=\\; \\frac{S + D + I}{N}\n",
    "$$\n",
    "\n",
    "In masked token prediction, the clause length usually remains the same (one predicted token per mask). Thus, the primary errors are substitutions. For example, if 3 tokens were masked and the model got 1 wrong,  \n",
    "$$\n",
    "  \\mathrm{WER} \\;=\\; \\frac{1}{3} \\approx 0.33.\n",
    "$$\n",
    "A WER of 0 indicates all masked tokens were correct; WER = 1.0 indicates every token in the clause was in error.  \n",
    "\n",
    "> *See WER definition and use in speech/MT evaluation*  \n",
    "> [Paper: Word Error Rate](https://www.researchgate.net/publication/271429169_Word_error_rates)  \n",
    "\n",
    "---\n",
    "\n",
    "### BERTScore for Masked Token Semantic Similarity\n",
    "\n",
    "**BERTScore** uses contextual embeddings to compare the predicted clause and the ground truth clause on a token level. Instead of strict string matches, it finds **embedding-based** matches, giving partial credit if the predicted token is semantically close. Let $\\mathbf{e}(t)$ be the contextual embedding of token $t$. Define:  \n",
    "\n",
    "- $\\lvert \\mathrm{cand}\\rvert$ = number of tokens in the **predicted clause** (usually same length as reference).  \n",
    "- $\\lvert \\mathrm{ref}\\rvert$ = number of tokens in the **ground truth clause**.  \n",
    "\n",
    "Compute:\n",
    "\n",
    "- **Precision₍BERT₎**  \n",
    "  $$\n",
    "    P_{\\mathrm{BERT}} \n",
    "    = \\frac{1}{\\lvert \\mathrm{cand} \\rvert}\n",
    "      \\sum_{j=1}^{\\lvert \\mathrm{cand} \\rvert}\n",
    "      \\max_{\\,i\\,}\\;\\cos\\bigl(\\mathbf{e}(\\mathrm{ref}_i),\\,\\mathbf{e}(\\mathrm{cand}_j)\\bigr)\n",
    "  $$\n",
    "\n",
    "- **Recall₍BERT₎**  \n",
    "  $$\n",
    "    R_{\\mathrm{BERT}} \n",
    "    = \\frac{1}{\\lvert \\mathrm{ref} \\rvert}\n",
    "      \\sum_{i=1}^{\\lvert \\mathrm{ref} \\rvert}\n",
    "      \\max_{\\,j\\,}\\;\\cos\\bigl(\\mathbf{e}(\\mathrm{ref}_i),\\,\\mathbf{e}(\\mathrm{cand}_j)\\bigr)\n",
    "  $$\n",
    "\n",
    "- **F₁₍BERT₎**  \n",
    "  $$\n",
    "    F_{\\mathrm{BERT}} \n",
    "    = 2 \\cdot \\frac{P_{\\mathrm{BERT}}\\;\\,R_{\\mathrm{BERT}}}\n",
    "                         {P_{\\mathrm{BERT}} + R_{\\mathrm{BERT}}}\n",
    "  $$\n",
    "\n",
    "Since most tokens in the clause are identical except for the masked ones, BERTScore focuses on how the **predicted tokens** align with the **ground truth tokens** in embedding space. If the model predicts a legal synonym or a morphologically close variant (e.g.\\ “δικαίωμα” vs. “δικαιώματος”), BERTScore will yield a high similarity even if the exact surface form differs.  \n",
    "\n",
    "> *See Zhang et al. (2019) for original BERTScore formulation*  \n",
    "> [Paper: BERTScore (Zhang et al., 2019)](https://arxiv.org/abs/1904.09675)  \n",
    "\n",
    "---\n",
    "\n",
    "### Sentence Embedding Cosine Similarity (Clause-Level Semantic Alignment)\n",
    "\n",
    "To evaluate the **overall meaning** of the filled clause, we can encode entire clauses into fixed vectors using **Sentence-BERT (SBERT)** or **Universal Sentence Encoder (USE)**, then compute cosine similarity between the **original clause** and the **predicted clause**. Formally, let $E(\\cdot)$ map a sentence to a dense vector. For the ground truth clause $r$ and predicted clause $c$:\n",
    "$$\n",
    "  \\mathrm{CosSim}_{\\mathrm{sent}}(c, r)\n",
    "  = \\frac{E(c)\\;\\cdot\\;E(r)}\n",
    "         {\\,\\lVert E(c)\\rVert \\;\\lVert E(r)\\rVert\\,}.\n",
    "$$\n",
    "A score near 1.0 means the two clauses are nearly identical in meaning, whereas a low score indicates the substituted token altered the legal proposition.  \n",
    "\n",
    "> *See Reimers & Gurevych (2019) for Sentence-BERT and STS evaluation*  \n",
    "> [Paper: SBERT (Reimers & Gurevych, 2019)](https://arxiv.org/abs/1908.10084)  \n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF Cosine Similarity (Lexical Overlap Baseline)\n",
    "\n",
    "As a simpler, lexicon-based baseline, represent each clause as a **TF-IDF-weighted bag-of-words vector**. Let $\\mathbf{v}_c$ and $\\mathbf{v}_r$ be the TF-IDF vectors for the **predicted** and **ground truth** clauses, respectively. Compute:\n",
    "$$\n",
    "  \\cos\\bigl(\\mathbf{v}_c,\\mathbf{v}_r\\bigr)\n",
    "  = \\frac{\\mathbf{v}_c \\;\\cdot\\;\\mathbf{v}_r}\n",
    "         {\\,\\lVert \\mathbf{v}_c\\rVert \\;\\lVert \\mathbf{v}_r\\rVert\\,}.\n",
    "$$\n",
    "If the model predicts the **exact** token, the vectors will match (cosine = 1). Any deviation in the masked token creates a gap in the corresponding TF-IDF dimension, reducing cosine similarity. Because the masked tokens are often high-IDF legal terms, TF-IDF cosine is a conservative measure: only nearly exact or highly overlapping clauses score close to 1.  \n",
    "\n",
    "> *See Salton et al. (1975) for the Vector Space Model and TF-IDF cosine*  \n",
    "> [Paper: Vector Space Model (Salton et al., 1975)](https://doi.org/10.1145/361219.361220)  \n",
    "\n",
    "---\n",
    "\n",
    "### Text to be masked\n",
    "\n",
    "```plaintext\n",
    "Άρθρο 1113. Κοινό πράγμα. — Αν η κυριότητα του [MASK] ανήκει σε περισσότερους  \n",
    "[MASK] αδιαιρέτου κατ΄ιδανικά [MASK], εφαρμόζονται οι διατάξεις για την κοινωνία.\n",
    "\n",
    "Άρθρο 1114. Πραγματική δουλεία σε [MASK] η υπέρ του κοινού ακινήτου. — Στο κοινό  \n",
    "[MASK] μπορεί να συσταθεί πραγματική δουλεία υπέρ του [MASK] κύριου άλλου ακινήτου  \n",
    "και αν ακόμη αυτός είναι [MASK] του ακινήτου που βαρύνεται με τη δουλεία. Το ίδιο ισχύει  \n",
    "και για την [MASK] δουλεία πάνω σε ακίνητο υπέρ των εκάστοτε κυρίων κοινού ακινήτου,  \n",
    "αν [MASK] από αυτούς είναι κύριος του [MASK] που βαρύνεται με τη δουλεία.\n",
    "```\n",
    "#### Ground Truth\n",
    "```plaintext\n",
    "Άρθρο 1113. Κοινό πράγμα: Αν η κυριότητα του πράγματος ανήκει σε περισσότερους  \n",
    "εξ αδιαιρέτου κατ' ιδανικά μέρη, εφαρμόζονται οι διατάξεις για την κοινωνία.\n",
    "\n",
    "Άρθρο 1114. Πραγματική δουλεία σε βάρος ή υπέρ του κοινού ακινήτου: Στο κοινό  \n",
    "ακίνητο μπορεί να συσταθεί πραγματική δουλεία υπέρ του εκάστοτε κυρίου άλλου ακινήτου  \n",
    "και αν ακόμη αυτός είναι συγκύριος του ακινήτου που βαρύνεται με τη δουλεία. Το ίδιο ισχύει  \n",
    "και για πραγματική δουλεία πάνω σε ακίνητο υπέρ των εκάστοτε κυρίων κοινού ακινήτου,  \n",
    "αν κάποιος από αυτούς είναι κύριος του ακινήτου που βαρύνεται με τη δουλεία.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T22:15:25.239193Z",
     "iopub.status.busy": "2025-06-08T22:15:25.239022Z",
     "iopub.status.idle": "2025-06-08T22:17:25.349473Z",
     "shell.execute_reply": "2025-06-08T22:17:25.348596Z",
     "shell.execute_reply.started": "2025-06-08T22:15:25.239177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "!pip install bert-score\n",
    "!pip install --upgrade transformers  accelerate bitsandbytes -q\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline\n",
    "from bert_score import score as bert_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import dotenv\n",
    "import json\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Define models and their types\n",
    "models = {\n",
    "    \"ilsp/Llama-Krikri-8B-Base\": \"clm\",\n",
    "    \"google/mt5-base\": \"enc-dec\",\n",
    "    \"google-bert/bert-base-multilingual-cased\": \"mlm\",\n",
    "    \"distilbert/distilbert-base-multilingual-cased\": \"mlm\",\n",
    "    \"FacebookAI/xlm-roberta-base\": \"mlm\",\n",
    "    \"microsoft/infoxlm-base\": \"mlm\",\n",
    "    \"nlpaueb/bert-base-greek-uncased-v1\": \"mlm\",\n",
    "    \"gealexandri/greeksocialbert-base-greek-uncased-v1\": \"mlm\",\n",
    "    \"AI-team-UoA/GreekLegalRoBERTa_v3\": \"mlm\",\n",
    "    \"ilsp/Meltemi-7B-v1.5\": \"clm\"\n",
    "}\n",
    "\n",
    "# Prepare masked texts and ground truths\n",
    "examples = [\n",
    "    {\n",
    "        \"masked_text\": \"Άρθρο 1113. Κοινό πράγμα. — Αν η κυριότητα του [MASK] ανήκει σε περισσότερους [MASK] αδιαιρέτου κατ΄ιδανικά [MASK], εφαρμόζονται οι διατάξεις για την κοινωνία.\",\n",
    "        \"ground_truth\": \"Άρθρο 1113. Κοινό πράγμα: Αν η κυριότητα του πράγματος ανήκει σε περισσότερους εξ αδιαιρέτου κατ' ιδανικά μέρη, εφαρμόζονται οι διατάξεις για την κοινωνία.\"\n",
    "    },\n",
    "    {\n",
    "        \"masked_text\": \"Άρθρο 1114. Πραγματική δουλεία σε [MASK] ή υπέρ του κοινού ακινήτου. — Στο κοινό [MASK] μπορεί να συσταθεί πραγματική δουλεία υπέρ του [MASK] κύριου άλλου ακινήτου και αν ακόμη αυτός είναι [MASK] του ακινήτου που βαρύνεται με τη δουλεία. Το ίδιο ισχύει και για την [MASK] δουλεία πάνω σε ακίνητο υπέρ των εκάστοτε κυρίων κοινού ακινήτου, αν [MASK] από αυτούς είναι κύριος του [MASK] που βαρύνεται με τη δουλεία.\",\n",
    "        \"ground_truth\": \"Άρθρο 1114. Πραγματική δουλεία σε βάρος ή υπέρ του κοινού ακινήτου: Στο κοινό ακίνητο μπορεί να συσταθεί πραγματική δουλεία υπέρ του εκάστοτε κύριου άλλου ακινήτου και αν ακόμη αυτός είναι συγκύριος του ακινήτου που βαρύνεται με τη δουλεία. Το ίδιο ισχύει και για πραγματική δουλεία πάνω σε ακίνητο υπέρ των εκάστοτε κυρίων κοινού ακινήτου, αν κάποιος από αυτούς είναι κύριος του ακινήτου που βαρύνεται με τη δουλεία.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T22:17:30.693393Z",
     "iopub.status.busy": "2025-06-08T22:17:30.693100Z",
     "iopub.status.idle": "2025-06-08T22:17:30.720296Z",
     "shell.execute_reply": "2025-06-08T22:17:30.719605Z",
     "shell.execute_reply.started": "2025-06-08T22:17:30.693367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from accelerate import infer_auto_device_map\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import LlamaConfig, AutoConfig\n",
    "\n",
    "# Function to normalize text (standardize punctuation)\n",
    "def normalize_text(text):\n",
    "    # Replace various punctuation with a standard space or colon\n",
    "    text = re.sub(r'[.:;—]', ' ', text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Function to get masked positions and ground truth words\n",
    "def get_masked_positions(masked_text, ground_truth):\n",
    "    # Normalize texts to avoid punctuation issues\n",
    "    masked_text_norm = normalize_text(masked_text)\n",
    "    ground_truth_norm = normalize_text(ground_truth)\n",
    "    \n",
    "    masked_words = masked_text_norm.split()\n",
    "    gt_words = ground_truth_norm.split()\n",
    "    \n",
    "    # Find [MASK] positions and align with ground truth\n",
    "    masked_positions = []\n",
    "    mask_count = masked_text.count(\"[MASK]\")\n",
    "    if mask_count == 0:\n",
    "        return masked_positions\n",
    "    \n",
    "    # Use SequenceMatcher to align sequences, allowing for multi-word matches\n",
    "    matcher = SequenceMatcher(None, masked_words, gt_words)\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag == 'replace' and i2 - i1 == 1 and masked_words[i1] == '[MASK]':\n",
    "            # [MASK] corresponds to ground truth words from j1 to j2\n",
    "            masked_positions.append((i1, ' '.join(gt_words[j1:j2])))\n",
    "    \n",
    "    return masked_positions\n",
    "\n",
    "# Prediction functions for different model types\n",
    "def predict_masks_mlm(model, tokenizer, masked_text):\n",
    "    inputs = tokenizer(masked_text, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    predicted_text = masked_text\n",
    "    for idx in mask_token_index:\n",
    "        predicted_id = torch.argmax(predictions[0, idx.item()]).item()\n",
    "        predicted_token = tokenizer.decode([predicted_id]).strip()\n",
    "        predicted_text = predicted_text.replace(\"[MASK]\", predicted_token, 1)\n",
    "    return predicted_text\n",
    "\n",
    "def predict_masks_clm(model, tokenizer, masked_text):\n",
    "    parts = masked_text.split(\"[MASK]\")\n",
    "    current_text = parts[0]\n",
    "    \n",
    "    for i in range(1, len(parts)):\n",
    "        # 1. Tokenize without moving to any device\n",
    "        inputs = tokenizer(current_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # 2. Identify the device of the first layer\n",
    "        first_device = list(model.hf_device_map.values())[0]\n",
    "        \n",
    "        # 3. Move each input tensor to that device\n",
    "        inputs = {k: v.to(first_device) for k, v in inputs.items()}\n",
    "        \n",
    "        # 4. Run the forward pass (Accelerate will move weights/activations as needed)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # 5. Get top prediction for the [MASK] position\n",
    "            next_token_logits = outputs.logits[0, -1]\n",
    "            predicted_id = torch.argmax(next_token_logits).item()\n",
    "            predicted_token = tokenizer.decode([predicted_id]).strip()\n",
    "        \n",
    "        current_text += predicted_token + parts[i]\n",
    "    \n",
    "    return current_text\n",
    "#def predict_masks_clm(model, tokenizer, masked_text):\n",
    "#    parts = masked_text.split(\"[MASK]\")\n",
    "#    current_text = parts[0]\n",
    "#    for i in range(1, len(parts)):\n",
    "#        inputs = tokenizer(current_text, return_tensors=\"pt\")\n",
    "#        with torch.no_grad():\n",
    "#            outputs = model(**inputs)\n",
    "#            # Allow multiple tokens to be generated\n",
    "#            predicted_ids = torch.topk(outputs.logits[0, -1], k=5).indices\n",
    "#            predicted_tokens = [tokenizer.decode([pid]).strip() for pid in predicted_ids]\n",
    "#            predicted_token = predicted_tokens[0]  # Take the most likely\n",
    "#        current_text += predicted_token + parts[i]\n",
    "#    return current_text\n",
    "\n",
    "def predict_masks_encdec(model, tokenizer, masked_text):\n",
    "    mask_count = masked_text.count(\"[MASK]\")\n",
    "    input_text = masked_text\n",
    "    for i in range(mask_count):\n",
    "        input_text = input_text.replace(\"[MASK]\", f\"<extra_id_{i}>\", 1)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=512)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    predicted_text = masked_text\n",
    "    for i in range(mask_count):\n",
    "        start_token = f\"<extra_id_{i}>\"\n",
    "        end_token = f\"<extra_id_{i+1}>\" if i + 1 < mask_count else None\n",
    "        if end_token:\n",
    "            pred_token = generated_text.split(start_token)[1].split(end_token)[0].strip()\n",
    "        else:\n",
    "            pred_token = generated_text.split(start_token)[1].strip()\n",
    "        predicted_text = predicted_text.replace(\"[MASK]\", pred_token, 1)\n",
    "    return predicted_text\n",
    "\n",
    "# Load models and tokenizers\n",
    "def load_model_and_tokenizer(model_name, model_type):\n",
    "    cache_dir = \"/kaggle/working/models\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "    if model_type == \"mlm\":\n",
    "        # -- MODEL --\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        # -- TOKENIZER --\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            token=hf_token,    \n",
    "        )\n",
    "    \n",
    "    elif model_type == \"clm\":\n",
    "         # 2. Create an empty model skeleton to compute device_map weights\n",
    "        with init_empty_weights():\n",
    "            dummy_config = AutoConfig.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                token=hf_token,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            dummy_model = AutoModelForCausalLM.from_config(dummy_config)\n",
    "        \n",
    "        # 3. Define per-device memory budget\n",
    "        max_memory = {\n",
    "            0: \"13GB\",    # GPU 0\n",
    "            1: \"13GB\",    # GPU 1\n",
    "            \"cpu\": \"29GB\" # CPU RAM\n",
    "        }\n",
    "        # 4. Compute device_map under these constraints\n",
    "        no_split = [\"LlamaDecoderLayer\", \"MistralDecoderLayer\"]\n",
    "        device_map = infer_auto_device_map(\n",
    "            dummy_model,\n",
    "            max_memory=max_memory,\n",
    "            no_split_module_classes=no_split  # avoid splitting between GPUs if needed\n",
    "        )\n",
    "#        model = AutoModelForCausalLM.from_pretrained(\n",
    "#        model_name,\n",
    "#        quantization_config=bnb_config,\n",
    "#        device_map=device_map,\n",
    "#        torch_dtype=\"auto\",\n",
    "#        low_cpu_mem_usage=True\n",
    "#    )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            token=hf_token,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=\"auto\",         # float16 on GPU if available\n",
    "            low_cpu_mem_usage=True       # reduce CPU peak-memory when deserializing\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            token=hf_token,\n",
    "        )\n",
    "    \n",
    "    elif model_type == \"enc-dec\":\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            token=hf_token,\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_token_metrics(predicted_text, ground_truth, masked_positions):\n",
    "    pred_words = normalize_text(predicted_text).split()\n",
    "    gt_words = normalize_text(ground_truth).split()\n",
    "    correct = 0\n",
    "    for pos, gt_phrase in masked_positions:\n",
    "        # Check if predicted phrase at position matches ground truth phrase\n",
    "        pred_phrase = pred_words[pos] if pos < len(pred_words) else ''\n",
    "        if pred_phrase == gt_phrase:\n",
    "            correct += 1\n",
    "    total_masks = len(masked_positions)\n",
    "    accuracy = correct / total_masks if total_masks > 0 else 0\n",
    "    return {\"Precision\": accuracy, \"Recall\": accuracy, \"F1\": accuracy}\n",
    "\n",
    "def compute_wer(predicted_text, ground_truth, masked_positions):\n",
    "    pred_words = normalize_text(predicted_text).split()\n",
    "    incorrect = 0\n",
    "    for pos, gt_phrase in masked_positions:\n",
    "        pred_phrase = pred_words[pos] if pos < len(pred_words) else ''\n",
    "        if pred_phrase != gt_phrase:\n",
    "            incorrect += 1\n",
    "    total_masks = len(masked_positions)\n",
    "    return incorrect / total_masks if total_masks > 0 else 0\n",
    "\n",
    "def compute_bertscore(predicted_text, ground_truth):\n",
    "    P, R, F1 = bert_score([predicted_text], [ground_truth], lang=\"el\", verbose=False)\n",
    "    return {\"BERTScore_P\": P.item(), \"BERTScore_R\": R.item(), \"BERTScore_F1\": F1.item()}\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def compute_sentence_similarity(predicted_text, ground_truth):\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    embeddings = model.encode([predicted_text, ground_truth])   # returns a NumPy array\n",
    "    sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return float(sim)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_tfidf_similarity(predicted_text, ground_truth):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([predicted_text, ground_truth])\n",
    "    sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    return float(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-03T00:44:11.875Z",
     "iopub.execute_input": "2025-06-03T00:44:11.521840Z",
     "iopub.status.busy": "2025-06-03T00:44:11.521505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "model_dir = \"/kaggle/working/models\"\n",
    "results = {}\n",
    "for model_name, model_type in models.items():\n",
    "    print(f\"Evaluating {model_name} ({model_type})\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, model_type)\n",
    "    model_results = []\n",
    "    for example in examples:\n",
    "        masked_text = example[\"masked_text\"]\n",
    "        ground_truth = example[\"ground_truth\"]\n",
    "        masked_positions = get_masked_positions(masked_text, ground_truth)\n",
    "        \n",
    "        # Predict based on model type\n",
    "        start = time.time()\n",
    "        if model_type == \"mlm\":\n",
    "            predicted_text = predict_masks_mlm(model, tokenizer, masked_text)\n",
    "        elif model_type == \"clm\":\n",
    "            predicted_text = predict_masks_clm(model, tokenizer, masked_text)\n",
    "        else:  # enc-dec\n",
    "            predicted_text = predict_masks_encdec(model, tokenizer, masked_text)\n",
    "        inference_time = time.time() - start\n",
    "        \n",
    "        # Compute metrics\n",
    "        token_metrics = compute_token_metrics(predicted_text, ground_truth, masked_positions)\n",
    "        wer = compute_wer(predicted_text, ground_truth, masked_positions)\n",
    "        bertscore = compute_bertscore(predicted_text, ground_truth)\n",
    "        sent_sim = compute_sentence_similarity(predicted_text, ground_truth)\n",
    "        tfidf_sim = compute_tfidf_similarity(predicted_text, ground_truth)\n",
    "        \n",
    "        model_results.append({\n",
    "            \"Example\": masked_text,\n",
    "            \"Predicted\": predicted_text,\n",
    "            \"Ground Truth\": ground_truth,\n",
    "            \"Token Metrics\": token_metrics,\n",
    "            \"WER\": wer,\n",
    "            \"BERTScore\": bertscore,\n",
    "            \"Sentence Similarity\": sent_sim,\n",
    "            \"TF-IDF Similarity\": tfidf_sim,\n",
    "            \"Inference Time\": inference_time\n",
    "        })\n",
    "    # Delete the model directory to free up space\n",
    "    if os.path.exists(model_dir):\n",
    "        try:\n",
    "            shutil.rmtree(model_dir)\n",
    "            print(f\"Deleted model directory: {model_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting model directory {model_dir}: {e}\")\n",
    "    results[model_name] = model_results\n",
    "\n",
    "# Display results\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    for i, res in enumerate(model_results):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Masked Text: {res['Example']}\")\n",
    "        print(f\"Predicted: {res['Predicted']}\")\n",
    "        print(f\"Ground Truth: {res['Ground Truth']}\")\n",
    "        print(f\"Token Metrics: {res['Token Metrics']}\")\n",
    "        print(f\"WER: {res['WER']:.2f}\")\n",
    "        print(f\"BERTScore: {res['BERTScore']}\")\n",
    "        print(f\"Sentence Similarity: {res['Sentence Similarity']:.2f}\")\n",
    "        print(f\"TF-IDF Similarity: {res['TF-IDF Similarity']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-03T00:40:05.330441Z",
     "iopub.status.idle": "2025-06-03T00:40:05.330687Z",
     "shell.execute_reply": "2025-06-03T00:40:05.330574Z",
     "shell.execute_reply.started": "2025-06-03T00:40:05.330564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract data for plotting (with inference times)\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build a flat list of all metrics, including the new Inference Time\n",
    "data = []\n",
    "for model_name, model_results in results.items():\n",
    "    for i, res in enumerate(model_results):\n",
    "        entry = {\n",
    "            'model': model_name,\n",
    "            'example': i+1,\n",
    "            'Token F1': res['Token Metrics']['F1'],\n",
    "            'WER': res['WER'],\n",
    "            'BERTScore F1': res['BERTScore']['BERTScore_F1'],\n",
    "            'Sentence Similarity': res['Sentence Similarity'],\n",
    "            'TF-IDF Similarity': res['TF-IDF Similarity'],\n",
    "            'Inference Time': res['Inference Time']\n",
    "        }\n",
    "        data.append(entry)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df_melted = pd.melt(df, id_vars=['model', 'example'], var_name='metric', value_name='value')\n",
    "\n",
    "# Ensure output dir\n",
    "output_dir = '/kaggle/working/Results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save JSON and CSV\n",
    "with open(os.path.join(output_dir, 'evaluation_results.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "df.to_csv(os.path.join(output_dir, 'evaluation_metrics.csv'), index=False)\n",
    "\n",
    "print(f\"Saved JSON, CSV, and plots (including Inference Time) under {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define model categories\n",
    "model_categories = {\n",
    "    'google/mt5-base': 'General multilingual',\n",
    "    'google-bert/bert-base-multilingual-cased': 'General multilingual',\n",
    "    'distilbert/distilbert-base-multilingual-cased': 'General multilingual',\n",
    "    'FacebookAI/xlm-roberta-base': 'General multilingual',\n",
    "    'microsoft/infoxlm-base': 'General multilingual',\n",
    "    'ilsp/Llama-Krikri-8B-Base': 'Greek general',\n",
    "    'nlpaueb/bert-base-greek-uncased-v1': 'Greek general',\n",
    "    'gealexandri/greeksocialbert-base-greek-uncased-v1': 'Greek general',\n",
    "    'ilsp/Meltemi-7B-v1.5': 'Greek general',\n",
    "    'AI-team-UoA/GreekLegalRoBERTa_v3': 'Greek legal'\n",
    "}\n",
    "\n",
    "df['category'] = df['model'].map(model_categories)\n",
    "\n",
    "# 2) Save Markdown table of the full DataFrame:\n",
    "with open(os.path.join(output_dir, 'df_metrics.md'), 'w', encoding='utf-8') as f:\n",
    "    f.write(df.to_markdown(index=False))\n",
    "\n",
    "# 3) Compute averages per model:\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "avg_model = df.groupby('model')[numeric_cols].mean().reset_index()\n",
    "with open(os.path.join(output_dir, 'average_metrics_per_model.md'), 'w', encoding='utf-8') as f:\n",
    "    f.write(avg_model.to_markdown(index=False))\n",
    "\n",
    "# 4) Plot average metrics per model:\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "metrics = ['Token F1','WER','BERTScore F1','Sentence Similarity','TF-IDF Similarity','Inference Time']\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    sns.barplot(data=avg_model, x='model', y=metric, ax=ax)     \n",
    "    ax.set_title(f'Avg {metric} per Model')\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(output_dir, 'Average_Metrics.png'), dpi=300)\n",
    "\n",
    "# 5) Plot inference time breakdown:\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(\n",
    "    data=df, x='model', y='Inference Time', hue='example',\n",
    "    orient='v'\n",
    ")                                                                   \n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'Inference_Time_per_Model_and_Example.png'))\n",
    "\n",
    "# 6) Plot all metrics per model and example in a single card\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 6 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    sns.barplot(data=df, x='model', y=metric, hue='example', ax=ax)\n",
    "    ax.set_title(f'{metric} per Model and Example', fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=10)\n",
    "    ax.set_ylabel(metric, fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    if idx == 0:\n",
    "        ax.legend(loc='upper right')\n",
    "    else:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# Remove unused subplots\n",
    "for j in range(n_metrics, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(output_dir, 'Per_Model_Metrics.png'))\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
